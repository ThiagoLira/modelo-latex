%% ------------------------------------------------------------------------- %%
\chapter{Conceitos}
l\label{cap:conceitos}


\section{Aprendizado Automático}

O campo de Aprendizado Automático (Machine Learning - ML) é um ramo da Ciência
da Computação que utiliza métodos estatísticos para criar sistemas que possam
aprender a realizar uma determinada tarefa através de dados. Um problema de ML possui
as seguintes partes \citep{dlbook}: \\

\begin{itemize}

  
\item A tarefa $T$, no caso dos dados da Intercement, um problema de regressão.
  Onde desejamos estimar valores numéricos a partir de uma matriz de dados $X$ e
  suas anotações numéricas $Y$.  
  
\item Uma métrica de performance $P$, que define a distância entre as predições
  do modelo e os dados de treinamento. A teoria por trás desses métricas será explicada na seção~\ref{sec:metricas}.

\item A experiência $E$, que define qual tipo de informação o modelo poderá usar
  durante o treinamento. Explicaremos esse conceito na seção~\ref{sec:exp} 
  
\end{itemize}

\subsection{Aprendizado}
\label{sec:exp}
Algoritmos de ML podem ser divididos nas categorias de aprendizado supervisionado, não supervisionado e aprendizado por reforço \citep{dlbook}. Técnicas dos dois primeiros tipos serão usadas para os dados da Intercement.


\subsubsection{Aprendizado Supervisionado}
\label{sec:apren} 
Aprendizado Supervisionado consiste, do ponto de vista estatístico, em aprender uma distribuição
de probabilidade do tipo $p(y | x)$. Nos são fornecidos diversos vetores de
entrada $x$ associados a uma anotação $y$ e gostariamos de modelar essa relação
entre dados e anotações para que possamos anotar novos dados inéditos para os
modelos. Ou seja, para diversos exemplos de vetores
$x$ são fornecidas anotações $y$, e desejamos então criar predições de anotações
$y'$ para novas entradas $x'$.


\subsubsection{Aprendizado Não Supervisionado}

Para o caso de Aprendizado Não Supervisionado, mantendo a estrutura do exemplo
anterior, desejaríamos então modelar uma distribuição do tipo $p(x)$, onde temos
também diversos exemplos de vetores aleatórios $x$ e estamos estudando alguma propriedade importante dessa distribuição.


\subsection{Definição do Problema}


Os dados da Intercement são anotados temporalmente, então podemos escolher trata-los como uma série temporal, i.e. a ordem de entrada dos dados no modelo importa. A tarefa de regressão se torna então prever próximas anotações desses dados num horizonte de tempo futuro finito. O problema ainda é de Aprendizado Supervisionado, mas iremos alterar a formalização do tópico~\ref{sec:apren} condicionando então uma anotação $y$ a parâmetros de entrada $x$, mas também ao histórico da série temporal, i.e. valores passados do objetivo $y$. A série temporal é composta por diversas medições indexadas pelo tempo $t$ do objetivo de predição $y$ e de um vetor $\textbf{x}$ de parâmetros. Os dados de treinamento serão os pares $(\{\textbf{x}_{t_0},y_{t_o}\},\{\textbf{x}_{t_1},y_{t_1}\}, \dots, \{\textbf{x}_{T},y_{T}\})$ no intervalo de tempo $[t_o,T]$. Seja um horizonte finito de tempo $F$, $F > T$, os modelos devem aprender uma distribuição de probabilidade do tipo:

\[ p(y_{T:F} | y_{t_{o}:T},\textbf{x}_{t_{0}:T}) \]

Cabe então fazermos a distinção entre modelos sequenciais e não-sequenciais \cite{dlbook}. Os modelos sequenciais irão usar a natureza temporal dos dados como descrita nesse tópico na modelagem e predições. Os modelos não-sequenciais irão tratar cada par $(\textbf{x},y)$ sem a sua anotação de tempo $t$.


\subsection{A Performance}
\label{sec:metricas}
\subsubsection{Estatística Frequentista e Estatística Bayesiana}
 
Como nesse trabalho serão usados métodos de inferência Bayesiana aplicados a ML,
cabe então uma breve elaboração das diferenças entre as duas principais
vertentes da estatística. O seguinte desenvolvimento é retirado de \cite{dlbook}:\\

Digamos que exista um evento aleatório que tenha um resultado com probabilidade
$p$ de acontecer. A visão frequentista de probabilidades diz que, se pudéssemos repetir infinitas vezes esse evento, a proporção de vezes que esse resultado irá acontecer se aproximará arbitrariamente de $p$. E então entenderíamos a probabilidade $p$ meramente como uma proporção de resultados positivos em uma certa amostra de experimentos. Mas e se o evento não pudesse ser repetido? Quando físicos criam modelos para explicar o nascimento do universo, é impossível pensar em repetir o Big Bang infinitas vezes para que se possam estimar probabilidades de certos eventos cosmológicos acontecerem. Nesse segundo caso, resultados são derivados de \textbf{graus de certeza}, onde a chance de um evento acontecer é estimada pela aplicação de conhecimentos prévios em vista de algo que foi observado posteriormente. A primeira maneira de se entender estatística é chamada de Frequentista e a segunda de Bayesiana. \\

E no campo de ML, as duas maneiras de se gerar predições são estimadores frequentistas e inferência Bayesiana \citep{dlbook}.

\subsubsection{Estimação por Log-verossimilhança}
 
Um exemplo de estimação frequentista que será usada nesse trabalho é a de
log-verossimilhança \citep{dlbook}. O desenvolvimento a seguir será feito para o
caso de distribuições de probabilidade que modelam problemas de aprendizado
\textbf{supervisionado}, e.g. $p(y|x)$. 
Essa estimação segue o princípio de maximizar a verossimilhança i.e. a probabilidade de
termos observado nossos dados \textit{com as anotações corretas}, parametrizada pelos parâmetros $\theta$: $p(y|x ; \theta)$.
Ou seja, dada uma matriz de dados $X$, suas anotações $Y$ e um conjunto de parâmetros $\theta$, o estimador de máxima verossimilhança de $\theta$ é dado por: \\

\[ \hat{\theta} = \argmax_{\theta} p_{modelo}(Y | X;\theta) \] 

Onde $p_{modelo}$ busca aproximar a real distribuição geradora dos dados $p$. Assumindo dados i.i.d e trocando a multiplicação por soma de logaritmos obtém-se: \\

\[ \hat{\theta} = \argmax_{\theta} \sum_{i=1}^{m} \log p_{modelo}(y^{(i)}| x^{(i)};\theta) \]

Maximizar a equação anterior é equivalente a minimizar a \textit{entropia-cruzada} entre
nossas anotações reais $y$ e as predições feitas pelo modelo $\hat{y}$
\citep{dlbook}. \\

Continuando com o caso de de aprendizado supervisionado, a \textit{entropia de uma variável aleatória} $y | x$ cuja função densidade
de probabilidade seja $p(y | x)$ pode ser calculada por \citep{shannon2001mathematical}: \\

\[ H(y | x)  = - \sum^m_{i=1} p(y_i | x_i)*\log p(y_i | x_i) \]

Ou ainda:

\[H(y | x) = - \mathop{\mathbb{E}}_p[\log p(y | x)] \]

Com o operador $\mathop{\mathbb{E}}_p[x]$ representando o valor esperado da
variável aleatória $x$ na distribuição $p$. \\

Sejam $p,p_{modelo}$ duas funções de distribuição de probabilidade. A \textit{entropia-cruzada} de $p_{modelo}$ em $p$ é definida pelo valor
esperado da entropia de $p_{modelo}$ na distribuição $p$ \citep{bayesml}: \\

\[H(p,p_{modelo}) =  \mathop{\mathbb{E}}_p[\log p_{modelo}(y | x)] \]

Se a distribuição $p_{modelo}$ for parametrizada pelo vetor $\theta$, a equação
acima toma a forma: \\


\[H(p,p_{modelo}) =  \mathop{\mathbb{E}}_p[\log p_{modelo}(y | x ; \theta)] \]

Minimizar essa equação é equivalente a maximizar a equação da
log-verossimilhança \citep{dlbook}. A sessão~\ref{sec:reglog} irá mostrar o caso
específico de estimação por log-verossimilhança para problemas de regressão.  



\subsubsection{Divergência KL}

A divergência KL, ou Kullback-Leibler, é uma medida de diferença entre duas distribuições de probabilidade (também chamada de Entropia Relativa). \\
Essa medida pode ser definida pelo valor esperado da diferença logarítmica da probabilidade dos dados serem observados em uma distribuição ou na outra \citep{dlbook}. \\
Sejam duas distribuições de probabilidade $p$ e $q$, bem como uma matriz de dados $X$: \\

\[D_{KL}(p || q) = \mathop{\mathbb{E}}[\log p(x) - \log q(x)]​\]

Ou em uma forma mais usual: \\

\[D_{KL}(p||q) = \sum_{i=1}^{N}p(x_{i}) \log \frac{p(x_{i})}{q(x_{i})}​\]


\subsubsection{Estimação por Log-verossimilhança para Regressão}
\label{sec:reglog}
Nos modelos usados nesse trabalho, a equação que irá ditar o \textbf{erro} a ser
minimizado pelos algoritmos é o MSE (explicado na sessão~\ref{sec:MSE}). 
É simples mostrar intuitivamente que minimizar o MSE entre os valores previstos
e os dados de treino pode ser considerado uma estimação por maximização de
verossimilhança \citep{dlbook}. É possível explicar uma regressão linear como
uma maximização de verossimilhança:
\\

Desejamos criar um modelo linear que explica a relação entre duas variáveis
$x$ e $y$ em um modelo da forma $y = \theta X + \epsilon$, onde $\epsilon
\sim \mathcal{N}(\epsilon;0,\,\sigma^2)$ e $\sigma$ é suposto conhecido.\\

Considera-se que esse modelo está produzindo uma distribuição condicional
$p(y|x)$ (assim como a maior parte dos problema de aprendizado supervisionado). \\ 

Define-se então, para esse modelo, $p(y|x) =
\mathcal{N}(y ; \hat{y}(x,\theta),\sigma^2)$, pois foi assumido que o ruído $\epsilon$
segue uma distribuição normal. Nota-se que a média da gaussiana é a previsão do
modelo. \\

A log-verossimilhança do modelo é dada por: \\

\begin{equation}
  \label{eq:lv}
\mathcal{L} =  \sum^m_{i=1}\log p(y_i | x_i ; \theta)
\end{equation}

E como definimos a distribuição gerada pelo modelo como $\mathcal{N}(y ;
\hat{y}(x,\theta),\sigma^2)$, então aplicando a definição da
distribuição normal na equação~\ref{eq:lv} chegamos em: \\


\begin{equation}
\label{eq:logver}
  \mathcal{L}= -m \log \sigma - \frac{m}{2} \log (2 \pi) - \sum^m_{i=1}
  \frac{\abs{\hat{y_i}-y_i}^2}{2 \sigma^2}
\end{equation}

Observa-se que essa equação que calcula a verossimilhança para regressão possui o termo $(\hat{y_i}-y_i^2)$, a diferença
quadrática entre as predições e os valores reais, o \textit{erro quadrático}. E
de fato, no desenvolvimento desse projeto, usaremos o Erro Quadrático Médio
(definido formalmente na seção~\ref{sec:MSE}) entre
predições e valores reais para calcular o erro do nosso modelo e otimiza-lo.  
E essa otimização pode ser interpretada como uma busca por parâmetros que
maximizem a verossimilhança dos valores gerados pelo modelo frente os dados
observados. \\ 

\subsection{Função de Custo}

A Função de Custo, derivada diretamente da verossimilhança,
é uma funcao usada para guiar os algoritmos de otimização na direcão de parâmetros que melhorem a performance do modelo \cite{dlbook}. Isso é feito, como explicado na Sessão~\ref{sec:reglog}, aproximando a distribuição gerada pelo modelo da distribuição observada pelos dados. \\

Nessa sessão iremos definir a função de custo usada em todos os modelos de regressão usados. \\

\subsubsection{Erro Quadrático Médio}
\label{sec:MSE}
O erro quadrático médio (em inglês, MSE) é uma medida de diferença entre duas variáveis contínuas
$\hat{y}$ e y \citep{cohen}. Ele é dado por: \\

\[MSE = \sum^n_{i=1}\frac{(y_i - \hat{y_i})^2}{n}\]

Essa métrica é uma medida absoluta. E quanto maior esse valor pior o desempenho
do modelo. \\


\subsection{Métrica de Acurácia}

Diferentemente da funcão de custo, a métrica de acurácia não é usada para guiar a otimizacão do modelo,
mas para testar as predições frente a realidade e avaliar sua qualidade. \\

\subsubsection{R-quadrado}
Como teste da acurácia dos modelos foi usada a métrica R-quadrado ($R^2$) \citep{cohen}. Sejam $\hat{y}$ e $y$ nossa previsão dada pelo modelo e o seu valor real, a acurácia do modelo é dada por:\\

\begin{align}
&R^2 = 1 - \frac{SS_{res}}{SS_{tot}} &\\
&SS_{tot} = \sum^n_{i=1} (y_i- \hat{y_i})^2 &\\
&SS_{res} = \sum^n_{i=1} (y_i - \bar{y})^2 &\\
&\bar{y} = \frac{1}{n} \sum^n_{i=1} y &
\end{align}

% \justify
$SS_{res}$ mensura a quantidade de variância \textit{residual}, a
variância entre os dados reais e as predições do modelo, i.e. a variância não
explicada pelo modelo. $SS_{tot}$ cumpre o papel da variância \textit{total}. A métrica $R^2$
então nos mostra quanto da variância dos dados é explicada pelo modelo. \\

Para essa métrica, o modelo pode ter um desempenho arbitrariamente ruim, com esse valor
podendo se tornar arbitrariamente negativo. Porém, seu valor máximo é 1,
indicando um modelo ideal.\\


\subsubsection{Custo quantílico}

O custo quantílico é usado na avaliacão de performance de predições temporais. \\
Dado um quantil $p \in (0,1)$ um valor real $y_{t}$ e uma predição $y^{*}_{t}(\rho)$, esse valor é definido por: \\

\begin{equation}
  \mathcal{QL}_{\rho}(y_{t},y^{*}_{t}(\rho)) =
\begin{cases*}
  2 \rho(y_{t} - y^{*}_{t}(\rho)) & se $ y_{t} - y^{*}_{t}(\rho) > 0$ \\
  2 (1 - \rho)(y^{*}_{t}(\rho) - y_{t}) & se $y_{t} - y^{*}_{t}(\rho) \leq 0$
\end{cases*}
\end{equation}




%% ------------------------------------------------------------------------- %%


\section{Modelos Usados} 

\subsection{Redes Neurais}

\label{sec:nn}
Redes neurais são aproximadores universais de funções \citep{nnuni}. Dado um problema
de classificação onde se deseja aprender uma função da forma $y = f^*(x)$, uma
Rede Neural define um mapeamento $y = f(x ; \theta)$, onde $\theta$ é o vetor de
parâmetros que serão aprendidos com o fim de minimizar a diferença entre a
distribuição empírica e essa distribuição gerada pelo modelo. Essa diferença pode
ser minimizada pelo método da verossimilhança, assim como elaborado na sessão~\ref{sec:reglog}.\\

O modelo neural é uma composição de funções que unem uma transformação linear e
a aplicação de uma função não-linear $\sigma$: \\

\[ f(x)=  \sigma(W*x + b) \]

Importante ressaltar que $W$ é uma matriz, assim como $x$ e $b$ são vetores. \\

A computação da saída de uma rede neural então pode ser escrita como:

\[   y = f_n \circ f_{n-1} \circ f_{n-2} \dots f_1(x)  \]

Para uma rede neural de $n$ \textbf{camadas}. Onde cada camada será uma função
$f_i$ cujos parâmetros são $W_i$ e $b_i$. Portanto, para a i-ésima camada da rede
sua saída será da forma: 

\[ f_i (x)=  a_i = \sigma(W_i*a_{i-1} + b_i) \]

Onde $a_{i-1}$ é a saída da camada anterior, também chamada de
\textbf{ativação}. A saída dessa camada é então a sua ativação $a_i$. \\ 

Vale notar que uma saída $y$ calculada por uma rede neural depende unicamente dos
seus parâmetros e da entrada $x$. Isso não será verdade para os modelos
\textbf{sequenciais} que também serão usados nesse trabalho, onde o estado
interno de computação desses modelos é usado como entrada para uma próxima
iteração \citep{dlbook}. \\

Na Figura~\ref{fig:nn} mostramos como seria uma rede que usa os índices RC3 e RC7 para
modelar como saída o índice RC28. \\  

\begin{figure}
  \centering
  \input{chapters/NN.tex}
  \caption{Um exemplo de Rede Neural no domínio dos dados da Intercement}
  \label{fig:nn}
\end{figure}


\bigskip


\subsection{Inferência Bayesiana em Machine Learning}
\label{sec:bayesinf}
O tratamento Bayesiano para modelos de ML é bastante diverso dos frequentistas \citep{dlbook}.
Em uma análise frequentista estima-se um valor de $\theta$ e então todas as
predições são feitas a partir desse valor. No caso Bayesiano se consideram todos
os possíveis valores de $\theta$ ao se fazer uma predição. É preciso especificar
um grau de certeza \textbf{a priori} $p(\theta)$ sobre os parâmetros, e então
consideramos que os dados \textbf{foram observados} e usamos a lei de Bayes para
calcular a \textbf{probabilidade} posterior $p(\theta | X,Y)$ usando para tal
$p(X,Y | \theta)$, a chamada verossimilhança \citep{bayesml}. 

\[    p(\theta | X,Y) = \frac{p(Y| X,\theta) p(\theta)}{p(Y)}   \]

Finalmente, para realizar uma inferência devemos integrar por toda a distribuição $p(\theta)$ marginalizando esse parâmetro. Se por exemplo queremos uma nova anotação $y^*$ para um novo dado $x^*$:


\begin{equation}
  \label{eq:int}
p(y^* | x^* , X,Y) = \int  p(y^* | x^*,\theta) p(\theta | X,Y)  d\theta 
\end{equation}

Essa integral é geralmente intratável pela dificuldade de se calcular
analiticamente $p(\theta | X,Y)$ \citep{ubertime}, devemos então tentar aproximar essa distribuição
pela chamada distribuição variacional, $q^*(\theta)$, e então resolvermos numericamente a Integral~\ref{eq:int}.

Esse processo chama-se Inferência Variacional e seu objetivo ( i.e. aproximar a distribuição variacional da distribuição real $p(\theta | X,Y)$)
é alcançado minimizando-se a seguinte inequação, o \textit{Variational Lower Bound}: \\

\begin{equation}
  \label {eq:ine}
  \mathcal{L}_{VI} = \int q(W) \log p(Y | X,W)dW - \mathnormal{KL}(q(W) ||p(W)) \\
                  \leq \log p(Y|X) 
\end{equation}


A Inferência Variacional pode ser aproximada em uma rede neural pela técnica do Monte Carlo Dropout \citep{dropbayes}. \\

O MC Dropout consiste no uso de \textbf{Dropout} em todas as camadas da rede
neural, i.e. descartar ativações aleatoriamente entre duas camadas da rede com
probabilidade $p$.\\

\begin{figure}
  \centering
  \input{chapters/dropout.tex}
  \label{fig:dropout}
  \caption{Representacão do uso de Dropout em uma rede neural}
\end{figure}

Formalmente, para um vetor de ativações $a$ na saída de uma
camada da rede neural, amostramos um vetor de \textit{máscara} $\epsilon$ de uma
distribuição de Bernoulli cujo parâmetro é $p$, i.e. $\epsilon \sim Bernoulli(p)$.
Finalmente, o novo vetor de ativações é calculado por $\epsilon * a$. O efeito desse
processo é de zerar aleatoriamente entradas do vetor de ativação.
\\

Seja uma rede neural com apenas uma camada, cujos parâmetros são $W$ e $b$, como explicado na Sessão~\ref{sec:nn}. Amostrar uma máscara para o vetor de ativação a cada computação da rede é equivalente a considerar que estamos amostrando os próprios pesos da rede a cada computação \cite{dropbayes}. Seja $W$ a matriz de pesos da rede neural, então, escrevemos que a cada computação essa matriz é calculada amostrando-se um vetor $\epsilon$ e multiplicando pelo valor atual dos pesos: \\

\newcommand{\diag}{\mathop{\mathrm{diag}}}

$$    W^*   \sim \mathnormal{q}(W) $$
$$    W^*  = \diag(\epsilon) * W $$


A função de custo de uma rede neural com dropout em todas as camadas pode ser escrita da seguinta forma:

\[     \mathcal{L}_{dropout} = \sum^N_{N=1} (y_n - \mathnormal{f}^{W^*,b}(x_n))^2 + \boldsymbol{\alpha}\{W^2,b^2\} \]


\cite{dropbayes} demonstram que a aplicação do dropout em todas as camadas da rede faz com que a minimização
do objetivo de treinamento seja equivalente a minimização da Inequação~\ref{eq:ine}, i.e. o objetivo da Inferência Variacional.\\

Então, após realizado o treinamento, a saída do nosso modelo é uma amostragem da distribuição $q(y^* | x^* , \theta)$.
Por conseguinte, para o cálculo da incerteza das predições do modelo se realizam $B$ computações estocásticas de uma mesma
saída $y^*$, sejam elas $Y^* = \{y^*_{(1)},y^*_{(2)}, \dots , y^*_{(B)}\}$. A incerteza
dessa medida é calculada então pela variância amostral de $Y^*$. E sua média pela média w- amostral.



\subsection{Random Forest}

Random Forests são um método de \textbf{Ensemble Learning} para classificação ou regressão. \textbf{Ensemble Learning} é uma família de técnicas no qual diversos modelos "fracos" são usados em conjunto com algum sistema de votação para que a a acurária do sistema em conjunto se torne melhor que a de qualquer um dos modelos sozinho. Seguindo essa ideia, Random Forests são conjuntos de diveras árvores de decisão simples unidas por um meta-algoritmo de votação para que se produza uma predição muito mais eficaz.


\section{Modelos Sequenciais}
Com o sucesso de modelos sequenciais no campo do Deep Learning, iremos averiguar se é possível modelar sequencialmente os dados da produção de cimento usando dois desses modelos. 
\\

\subsection{Rede Neural Recorrente}
% \input{tiks/RNNSimplified.tex}

A família das Redes Neurais Recorrentes (RNN) é composta por modelos especializados
em processar dados sequenciais \citep{dlbook}, da forma $x^{(1)},x^{(2)} ,x^{(3)}\dots ,x^{(T)}$. Uma rede neural recorrente é definida por uma função com
\textbf{recorrência} ou \textbf{recursão}, de modo que no processamento de uma
sequência o estado da rede seja de certo modo propagado temporalmente. A equação
a seguir ilustra uma função com recorrência: \\

\[h^{(t)} = f(h^{(t-1)},x^{(t)};\theta)\]

Nessa equação notamos que na iteração $t$ o valor do vetor $h$ depende de
$h_{t-1}$.RNNs aprendem a usar esse vetores como
\say{resumos} das iterações passadas. Desse modo , o modelo ganha a capacidade
de usar informações passadas da sequência para o cálculo
de uma saída. Essa capacidade porém começa a ser dificultada quando o modelo
realiza muitas iterações temporais. Durante o aprendizado, sequências muito
extensas criam problemas numéricos para o cálculo dos gradientes, esses podendo
tomar valores muito pequenos ou muito grandes. Foram propostos outros modelos
que não possuam esse problema, como o modelo LSTM, explicado na sessão~\ref{sec:lstm}. 

%%%
\begin{figure}[H]
\centering
\includegraphics[width=0.9\columnwidth]{rnn.png}
\caption{Grafo de Computação de uma RNN genérica \citep{dlbook}}
\label{fig:rnngraph}
\end{figure}
%%%

Como podemos ver na Figura~\ref{fig:rnngraph}, a entrada $x$, ao lado do estado
interno $h$, são usados para calcular um novo estado. Nota-se que o vetor $h$,
bem como os parâmetros internos da RNN, são divididos entre iterações temporais.
\\

Essa classe de modelos normalmente é usada para modelagem de linguagem. Buscando
estimar uma distribuição de probabilidade $p(w_t | w_{t-1},w_{t-2},w_{t-3} \dots
) $ onde os $w_i$ são palavras subsequentes de um texto. Normalmente um modelo
dessa natureza busca resolver um problema de classifição, onde a próxima palavra
a ser prevista pelo modelo é uma entre todas as possibilidades de um certo
vocabulário. No caso do domínio em questão desejamamos resolver um problema de
regressão, onde nosso alvo é um valor numérico. Para treinar um desses modelos,
precisamos usar como entrada exemplos subsequentes de dados, onde cada exemplo
de entrada tem um exemplo pareado de saída. Basicamente redes neurais
recorrentes funcionam recebendo um exemplo de entrada, criando uma representação
interna com o mesmo e então gerando uma saída e comparando essa saída com o
exemplo de saída real, gerando um erro. Finalmente, esse erro é propagado para
alterar seus parâmetros (com o fim de achar um conjunto de parâmetros que gere
boas previsões). \\ 


Como já explicado anteriormente, nossos dados de entrada e saída não estão necessariamente pareados perfeitamente dia a dia. Portanto, foi necessário achar intervalos de tempo nos dados onde existe esse pareamento. Isso reduz drasticamente quais períodos representados nos dados realmente podem ser usados para treinar um desses modelos.


\subsubsection{LSTM}
\label{sec:lstm}

LSTMs \citep{lstm} são um tipo de RNN que por meio de sua arquitetura permitem que sequências
maiores sejam processadas sem que o fluxo dos gradientes propagados pela rede se torne
numericamente problemático (i.e. tendendo a 0 ou a infinito). A
Figura~\ref{fig:lstm} ilustra o fluxo dos sinais em uma LSTM. Observa-se que a
entrada $x_t$ e o antigo estado $h_{t-1}$ são usados para o cálculo de diversos \textit{gates} ou \say{portas}, e
finalmente um novo estado $h_t$ é gerado.\\

\begin{figure}
\centering
\caption{Diagrama da arquitetura de uma LSTM}
\input{chapters/lstm.tex}
\label{fig:lstm}
\end{figure}




Uma rede LSTM possui três \say{portas}. Cada porta possui duas matrizes $W,U$ e um
vetor $b$ de parâmetros. Uma iteração da LSTM começa com o cálculo dos sinais
$o_t,i_t,f_t$.\\

\[f_t = \sigma_g(W_fx_t + U_fh_{t-1} + b_f)\]
\[i_t = \sigma_g(W_ix_t + U_ih_{t-1} + b_i)\]
\[o_t = \sigma_g(W_ox_t + U_oh_{t-1} + b_o)\]

O diferencial de uma LSTM é a propagação do sinal $c_t$, a célula de memória.
Esse valor depende de $f_t$ e $i_t$, que influenciam em como o valor da
célula de memória será atualizado na presente iteração. A equação a seguir
mostra como o valor da célula de memória é calculado. Onde $\circ$ é o produto Hadamard, ou apenas multiplição entrada por entrada de
duas matrizes ou vetores. \\

\[c_t = f_t \circ c_{t-1} + i_t \circ \sigma_c(W_cx_t + U_ch_{t-1} + b_c)\]

Nota-se que $f_t$
define quanto do valor antigo da célula de memória deve participar no cálculo do
seu novo valor. 
Da mesma maneira $i_t$ define quanto da nova entrada deve ser usada no cálculo desse valor.
Em outras palavras, as portas $i_t$ e $f_t$ definem o quanto a LSTM deve,
respectivamente, \say{lembrar} e \say{esquecer}.


O novo estado da LSTM é então calculado por: \\
\[h_t = o_t \circ \sigma_h(c_t)\]




\subsubsection{Rede Encoder-Decoder}
\label{sec:encdec}
Redes Encoder-Decoder são usadas para modelagem de sequência para
sequência, ou seja, para receber dados sequenciais como entrada e gerar
sequências como saída \citep{dlbook}. Esses modelos possuem duas partes, ambas compostas por
RNNs. \\

\begin{figure}[H]
\centering
\input{chapters/encdec.tex}
\caption{ Diagrama de Rede Encoder-Decoder.\\ Modificado de \cite{encdec}}

\end{figure}
  
O \textbf{encoder} é uma RNN que busca receber uma sequência de entrada de
tamanho arbitrário e gerar uma representação como saída, o seu estado interno,
representando em marrom no diagrama acima. O \textbf{Decoder} então recebe essa representação interna (também chamada
de \textit{codificação}) e usa-a para gerar saídas sequencialmente, podendo
então gerar essas sequências de duas formas: O Decoder pode usar as suas próprias saídas
como entrada para gerar a saída da próxima iteração temporal, ou então usar dados de treinamento como
entradas, essa última forma chamada de \textit{teacher forcing}. No diagrama
está indicado o acoplamento entre o decoder e o encoder. Essa codificação é apenas
um vetor (cuja dimensão é um \textit{hiper-parâmetro} de treinamento) que resume a
informação sequencial lida pelo encoder e a transmite para o decoder.
Hiper-parâmetros são parâmetros do problema de aprendizado que são definidos
pelo programador e não fazem parte dos valores aprendidos pelo algoritmo de
aprendizado. É comum que se realizem diversos experimentos com diferentes
valores de hiper-parâmetros e se escolha os que geraram melhores resultados. 
\\

Redes encoder-decoder são muito usadas para aprender \textit{representações} para os
dados. Isso quer dizer que as codificações da rede encoder-decoder são frutos de
transformações no espaço das entradas que buscam extrair a informação mais útil
para o objetivo do aprendizado. Podemos pensar nessas transformações como
mudanças de coordenadas nas quais estamos representando os dados. Justamente o problema de aprender
representações estimulou desde 2006 uma redescoberta do DL \citep{dlbook}. Criar
operações que transformem os dados de entrada de modo a facilitar o aprendizado
pode levar anos se essa tarefa for colocada na mão de especialistas humanos
\citep{dlbook}, então é uma capacidade valiosa proporcionada por algoritmos de
DL como redes encoder-decoder. \\



\subsubsection{Modelo Encoder-Decoder-Forecaster}

Iremos reproduzir nesse trabalho o modelo proposto em \cite{ubertime}. A
arquitetura consiste em uma rede \textbf{encoder-decoder} que aprende codificações da
série temporal (i.e. uma representacão que extraia informações úteis para o
problema) e então uma rede \textbf{forecaster} que usa essas codificações ao lado de
variáveis exógenas a série temporal para realizar predições.  


\begin{figure}[H]
\centering
\includegraphics[width=0.9\columnwidth]{uber.png}
\caption{Arquitetura do modelo proposto por \cite{ubertime}}
\end{figure}


Durante o pré-treinamento a rede encoder-decoder consome sequências de $F + T$ dias
da série temporal. O encoder cria uma representação vetorial $h$ depois de
receber como entrada os primeiros $T$ dias da sequência. Então, $h$ é usado como
inicialização do estado interno do decoder, e esse então consume mais $F$
entradas da sequência. Para o decoder, se em uma iteração sua entrada é $X_i$,
então sua saída será comparada com $X_{i+1}$, e esse erro é propagado após lidas
todas as entradas para que a representação $h$ proveniente do encoder possa se
tornar mais informativa.


Esse modelo também possui outra característica importante. Todas as camadas da
redes neurais que compõe o encoder, o decoder e o forecaster possuem
Dropout com probabilidade $p$. Ou seja, podemos usar a técnica do Monte
Carlo Dropout para estimar a variância de cada predição feita por esse modelo. A
rede então se torna um modelo bastante robusto para consumir séries temporais,
assim realizando predições e incertezas.\\


Essa capacidade do modelo o torna mais interessante do que meramente usar uma
rede LSTM para o problema de regressão. Agora iremos definir como a informação
de incerteza pode ser calculada usando esse modelo. \\

Na seção~\ref{sec:reglog} definimos um problema de regressão linear sob o ponto
de vista Bayesiano. Para esse problema assumimos a distribuição geradora de
predições como $p(y |f^W(x))$. Onde $f^W(.)$ é a rede Encoder-Forecaster após o treino.
Como estamos lidando com regressão, podemos especificar ainda essa distribuição
como \citep{ubertime}:  

\begin{equation}
  \label{eq:reg}
 y| W \sim \mathcal{N}(y;f^W(x),\,\sigma^2)
\end{equation}

 Na seção~\ref{sec:bayesinf} explicamos como usamos inferência
 Bayesiana para marginalizar os parâmetros $W$ e calcular a distribuição $p(y^*
 | x^*)$ pela Integral~\ref{eq:int}. A variância dessa última distribuição quantifica a incerteza das predições
 \citep{dropbayes}, e pode ser decomposta usando a lei da variância total \citep{ubertime}: \\



\[ Var(y^* | x^*) = Var[\mathop{\mathbb{E}}(y^* | W,x^*)] +
  \mathop{\mathbb{E}}[Var(y^* | W,x^*)] \]

Como o primeiro termo dessa soma dentro do operador do valor esperado segue uma distribuição normal
(Equação~\ref{eq:reg}), podemos substituí-lo pela sua média. O
segundo termo será definido como $\sigma^2$: \\ 
 
\[ Var(y^* | x^*) = Var(f^W(x^*)) + \sigma^2 \] 


Vemos que nossa incerteza é decomposta em dois termos. $Var(f^W(x^*))$ denota a
nossa ignorância em relação aos parâmetros do modelo, referida como
\textit{incerteza do modelo}. $\sigma^2$ seria então o \textit{ruído inerente} ao
processo gerador de dados.
 
A estimativa de $Var(f^W(x^*))$ se dá pela técnica do MC Dropout. A variância
será aproximada pela variância amostral de $B$ predições estocásticas calculadas
na rede com camadas de Dropout ativadas. Seja $\{\hat{y}^*_{(1)},\hat{y}^*_{(2)}, \dots
\hat{y}^*_{(B)}\}$ o vetor de predições amostrado dessa forma, e $\hat{y}^*$ sua
média amostral, temos: \\

\[   Var(f^W(x^*))  = \frac{1}{B}\sum^B_{B=1}(\hat{y}^*_{(B)} - \hat{y}^*)^2  \]


O ruído inerente das medidas é estimado usando os \textit{dados de validação},
i.e. aqueles não
usados durante o treinamento. Como
proposto por \citep{ubertime}, sejam $(X',Y')$ nossos dados de validação, com
$V$ entradas. Esses dados são independentes do nosso modelo
treinado $f^W(.)$, então podemos usa-los para estimar o ruído inerente do
modelo: \\

\[ \hat{\sigma}^2 = \frac{1}{V}\sum^V_{V=1}(y'_v - f^W(x'_v))^2 \]

Finalmente, sejam nossa incerteza do modelo $\eta_1^2$ e o ruído inerente
$\eta^2_2$, calcula-se a incerteza total do modelo por: \\

\[  \eta_{tot} = \sqrt{  \eta_1^2 + \eta_2^2 }  \]


\subsection{Modelo DeepAR}

O modelo \textit{DeepAR}, proposto em \cite{deepar}, é baseado em redes neurais recorrentes auto-regressivas,
i.e. que tem sua saída realimentada na entrada da iteração posterior. Por meio da função de verossimilhança binomial esse modelo é capaz
de realizar predições probabilísticas.

O modelo é descrito pela seguinte equação, onde $h$ é uma RNN implementada com células de LSTMs:

\[
h_{i,t} = h(h_{i,t-1},y^*_{i,t-1},x_{i,t}, \theta)
\]

Nota-se que o estado da RNN tem como argumento o estado anterior e os parâmetros de entrada, mas também a saída da iteração passada do modelo, $y^*_{i,t-1}$. \\

A verossimilhança do modelo, $\mathcal{L}(y_{i,t} | \omega(h_{i,t}))$, é uma distribuição fixa cujos parâmetros são dados por uma função $\omega$ da saída $h$ do modelo. Usaremos uma função de verossimilhança binomial já que estamos otimizando um objetivo de regressão, assim como na Equação~\ref{eq:logver}:

\[
\mathcal{L}(y | \mu,\sigma) = {(2\pi\sigma^2)}^{-\frac{1}{2}} \exp(-  \frac{-y - \mu^2}{2\sigma^2})  
\]

A média $\mu$ e o desvio-padrão $\sigma$ são calculados diretamente pela saída do modelo. Para a média, usamos uma transformação linear parametrizada por $W_{\mu},b_{\mu}$. No caso do desvio-padrão, para garantirmos que ele seja maior que 0, usamos uma função softplus após uma transformação linear análoga ao caso da média, parametrizada por  $W_{\sigma},b_{\sigma}$.\\

\[ \mu(h_{i,t}) = W_{\mu}h_{i,t} + b_{\mu} \]

\[ \sigma(h_{i,t}) = \log(1 + \exp(W_{\sigma}h_{i,t}+ b_{\sigma})) \]


\subsubsection{Treinamento}



O modelo possui uma etapa de treinamento e uma etapa de validação. O Diagrama~\ref{dig:dar} ilustra como o modelo se comporta em cada um desses momentos: \\


TO-DO-IMAGEM \\


O funcionamento do modelo é similar ao de uma rede Encoder-Decoder, como visto na Sessão~\ref{sec:encdec}, o modelo deve consumir intervalos de $F$ valores dos parâmetros da série temporal, essa seria a fase do \textit{encoding}, para então gerar $T$ anotações dos dados na fase do \textit{decoding}. Durante o treinamento o erro do modelo é calculado comparando-se uma anotação gerada durante a fase de encoding $y^{*}_{t}$ com o seu valor real $y_{t}$. \\

Durante o \textbf{treinamento} o modelo recebe como entrada do instante $t$, o valor real da série no instante $t-1$. \\

Durante a \textbf{validação} o modelo é \textbf{realimentado} com a anotação $y^{*}_{t-1}$ gerada na iteração anterior. \\ 



O treinamento do modelo é feito pela maximização da log-verossimilhança: \\

\[
\theta = argmax \sum_i{\log(\mathcal{L}(y_i | \omega(h_{i})))}
\]


Uma saída $y^*_{i,t}$ do modelo é calculada por meio de uma amostragem da Gaussiana determinada pela média e variância calculadas: \\

\[
 y^*_{i,t} \sim \mathcal{N}(\mu,\sigma) 
\]

\subsection{Modelo com Deep Factors}

O modelo \textit{Deep Factors with Gaussian Process} \cite{deepfactors} separa o problema de predição em uma parte \textbf{local}, modelada por um Processo Gaussiano que gera incertezas, e uma parte \textbf{fixa}, modelada por uma rede neural recorrente treinada em diversas séries temporais do mesmo domínio.

\subsubsection{Processo Gaussiano}

Um Processo Gaussiano é uma coleção de variáveis aleatórias,
sendo que qualquer subconjunto finito das mesmas é descrito por uma Distribuição Gaussiana conjunta \cite{gpml}. \\

Um Processo Gaussiano é descrito completamente pela sua função de média e sua função de covariância. Usamos essas duas funções para amostrar o processo estocástico $f(x)$ que está sendo modelado: \\

\[
f(x) \sim \mathcal{GP}( m(x), K(x,x'))
\]

É comum que se escolha como função média apenas a constante 0 \cite{gpml}.  A função de covariância, as vezes chamada de Kernel, deve especificar a covariância entre os pares de variáveis aleatórias (i.e. os dados), para problemas de regressão é usual escolhermos um Kernel que relacione uma medida de distância das variáveis de entrada. Como por exemplo o \textit{quadrado da função exponencial}: \\

\[
  K(x,x') = \exp(-\frac{1}{2}\abs{x - x'}^2)
\]


\subsubsection{Modelo Generativo}

O modelo Deep Factors então é composto por duas partes, a combinação linear de ambas é a saída do modelo i.e. a emissão de valores de saída amostrados: \\


\[
    \textbf{Gerador de ruído local: }  r_i \sim \mathcal{GP} (0, K_i(.,.))
\]

O ruído é gerado por um Processo Gaussiano com média 0 e função de covariância exponencial. O Processo calcula incertezas em todos os pontos $x_{i,t}$ da série temporal $i$.
  
\[
  \textbf{Gerador da parte fixa aprendida globalmente: }  f_{i,t} = W_ig_t(x_{i,t})
\]
A função $g_t$ é uma RNN, no caso desse trabalho uma célula LSTM, que recebe como entrada um certo número $T$ de passos da série temporal, e então emite como saída um horizonte finito $F$ de predições.
\[
  \textbf{Emissão: }  y^*_{i,t} \sim p(. | u_{i,t}) , u_{i,t} = r_{i,t} + f_{i,t}  
\]

\subsubsection{Treinamento}

Como $p(. | u_{i,t})$ é distribuida normalmente, não precisamos de inferência para calcular a verossimilhança marginal do modelo, que é dada por: \\

\[
p(y_{i}) = \mathcal{N}(f_i,K_i + \sigma_i^2\mathcal{I})
\]

O objetivo de treinamento é achar o conjunto de parâmetros da RNN e do Processo Gaussiano que minimizem a log-verossimilhança conjunta dos dados: \\

\[
\theta = argmax \sum_i{\log(p(y_i))}
\]


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../quali"
%%% TeX-command-default: "latexmk"
%%% bibtex-file-path: "../bibliografia"
%%% End: