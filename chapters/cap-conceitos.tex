%% ------------------------------------------------------------------------- %%
\chapter{Conceitos}
\label{cap:conceitos}


\section{Aprendizado Automático}

O campo de Aprendizado Automático (Machine Learning - ML) é uma ramo da Ciência da Computação que se arma de métodos estatísticos para criar sistemas que podem aprender (i.e. melhorar sua acurácia) através de dados.

Algoritmos de ML podem ser divididos nas categorias de aprendizado supervisionado, não supervisionado e aprendizado por reforço \citep{dlbook}. Técnicas dos dois primeiros tipos serão usadas para os dados da Intercement.

\subsection{Aprendizado Supervisionado}

Aprendizado Supervisionado consiste, do ponto de vista estatístico. em aprender uma distribuição
de probabilidade do tipo $p(y | x)$. Nos são fornecidos diversos vetores de
entrada $x$ associados a uma anotação $y$. Gostariamos de modelar essa relação
entre dados e anotacoes para que possamos anotar novos dados inéditos para os modelos.


ou seja, para diversos exemplos de vetores
$x$ são fornecidas anotações $y$, e desejamos então criar predições de anotacoes
$y'$ para novas entradas $x'$.


\subsection{Aprendizado Não Supervisionado}

Para o caso de Aprendizado Não Supervisionado, mantendo a estrutura do exemplo
anterior, desejaríamos então modelar uma distribuição do tipo $p(x)$, onde temos
também diversos exemplos de vetores aleatórios $x$ e estamos estudando alguma propriedade importante dessa distribuição.

\section{Estatística Frequentista e Estatística Bayesiana}

Como nesse trabalho serão usados métodos de inferência Bayesiana aplicados a ML,
cabe então uma breve elaboração das diferenças entre as duas principais da estatística. O seguinte desenvolvimento é retirado de \cite{dlbook}:\\

Digamos que exista um evento aleatório que tenha um resultado com probabilidade
$p$ de acontecer. A visão frequentista de probabilidades diz que, se pudessemos repetir infinitas vezes esse evento, a proporção de vezes que esse resultado irá acontecer se aproximará de arbitrariamente de $p$. E então entenderíamos a probabilidade $p$ meramente como uma proporção de resultados positivos em uma certa amostra de experimentos. Mas e se o evento não pudesse ser repetido? Quando físicos criam modelos para explicar o nascimento do universo, é impossível pensar em repetir o Big Bang infinitas vezes para que se possam estimar probabilidades de certos eventos cosmologicos acontecerem. Nesse segundo caso, resultados são derivados de \textbf{graus de certeza}, onde a chance de um evento acontecer é estimada pela aplicação de conhecimentos prévios em vista de algo que foi observado posteriormente. A primeira maneira de se entender estatística é chamada de Frequentista e a segunda de Bayesiana. \\

E no campo de ML, as duas maneiras de se gerar predições são estimadores frequentistas e inferência Bayesiana \citep{dlbook}.

\subsection{Estimação por Log-verossimilhança}
 
Um exemplo de estimação frequentista que será usada nesse trabalho é a de
log-verossimilhança \citep{dlbook}. O desenvolvimento a seguir será feito para o
caso de distribuições de probabilidade que modelam problemas de aprendizado
\textbf{supervisionado}, e.g. $p(y|x)$. 
Essa estimação segue o princípio de maximizar a verossimilhança i.e. a probabilidade de
termos observado nossos dados \textit{com as anotações corretas}, parametrizada pelos parâmetros $\theta$: $p(y|x ; \theta)$.
Ou seja, dada uma matriz de dados $X$, suas anotações $Y$ e um conjunto de parâmetros $\theta$, o estimador de máxima verossimilhança de $\theta$ é dado por: \\

\[ \hat{\theta} = \argmax_{\theta} p_{modelo}(Y | X;\theta) \] 

Onde $p_{modelo}$ busca aproximar a real distribuição geradora dos dados $p$. Assumindo dados i.i.d e trocando a multiplicação por soma de logaritmos obtem-se: \\

\[ \hat{\theta} = \argmax_{\theta} \sum_{i=1}^{m} \log p_{modelo}(y^{(i)}| x^{(i)};\theta) \]

Maximizar a equação anterior é equivalente a minimizar a \textit{entropia-cruzada} entre
nossas anotações reais $y$ e as predições feitas pelo modelo $\hat{y}$
\citep{dlbook}. \\

Continuando com o caso de de aprendizado supervisionado, a \textit{entropia de uma variável aleatória} $y | x$ cuja função densidade
de probabilidade seja $p(y | x)$ pode ser calculada por \citep{shannon2001mathematical}: \\

\[ H(y | x)  = - \sum^m_{i=1} p(y_i | x_i)*\log p(y_i | x_i) \]

Ou ainda:

\[H(y | x) = - \mathop{\mathbb{E}}_p[\log p(y | x)] \]

Com o operador $\mathop{\mathbb{E}}_p[x]$ representando o valor esperado da
variável aleatória $x$ na distribuição $p$. \\

Sejam $p,p_{modelo}$ duas funções de distribuição de probabilidade. A \textit{entropia-cruzada} de $p_{modelo}$ em $p$ é definida pelo valor
esperado da entropia de $p_{modelo}$ na distribuição $p$ \citep{bayesml}: \\

\[H(p,p_{modelo}) =  \mathop{\mathbb{E}}_p[\log p_{modelo}(y | x)] \]

Se a distribuição $p_{modelo}$ for parametrizada pelo vetor $\theta$, a equação
acima toma a forma: \\


\[H(p,p_{modelo}) =  \mathop{\mathbb{E}}_p[\log p_{modelo}(y | x ; \theta)] \]

Minimizar essa equação é equivalente a maximizar a equação da
log-verossimilhança \citep{dlbook}. A sessão~\ref{sec:reglog} irá mostrar o caso
específico de estimação por log-verossimilhança para problemas de regressão.  

\subsubsection{Estimação por Log-verossimilhança para Regressão}
\label{sec:reglog}
Nos modelos usados nesse trabalho, a equação que irá ditar o \textbf{erro} a ser
minimizado pelos algoritmos é o MSE (explicado na sessão~\ref{sec:MSE}). Porém,
é simples mostrar intuitivamente que minimizar o MSE entre os valores previstos
e os dados de treino pode ser considerado uma estimação por maximização de
verossimilhança \citep{dlbook}. É possível explicar uma regressão linear na ótica de maximizar uma verossimilhança:
\\

Desejamos criar um modelo linear que explica a relação entre duas variáveis
$x$ e $y$ em um modelo da forma $y = \theta X + \epsilon$, onde $\epsilon
\sim \mathcal{N}(\epsilon;0,\,\sigma^2)$ e $\sigma$ é suposto conhecido.\\

Considera-se que esse modelo está produzindo uma distribuição condicional
$p(y|x)$ (assim como a maior parte dos problema de aprendizado supervisionado). \\ 

Define-se então, para esse modelo, $p(y|x) =
\mathcal{N}(y ; \hat{y}(x,\theta),\sigma^2)$, pois foi assumido que o ruído $\epsilon$
segue uma distribuição normal. Nota-se que a média da gaussiana é a previsão do
modelo. \\

A log-verossimilhança do modelo é dada por: \\

\[ LV =  \sum^m_{i=1}\log p(y_i | x_i ; \theta)\]


E como definimos a distribuição gerada pelo modelo como $
\mathcal{N}(y ; \hat{y}(x,\theta),\sigma^2)$, então usando a definição da
distribuição normal: \\

\[LV = -m \log \sigma - \frac{m}{2} \log (2 \pi) - \sum^m_{i=1}
  \frac{\abs{\hat{y_i}-y_i}^2}{2 \sigma^2} \]

Essa equação, ao ser otimizada, chega na mesma estimação de parâmetros $\theta$
que usar a equação do Erro Quadrático Médio \citep{dlbook}, explicada na sessão~\ref{sec:MSE}.\\


\section{Métricas de Acurácia}

\subsection{R-quadrado}
Como teste da acurácia dos modelos foi usada a métrica R-quadrado \citep{cohen}. Sejam $\hat{y}$ e $y$ nossa previsão dada pelo modelo e o seu valor real, a acurácia do modelo é dada por:\\

\[R_2 = 1 - \frac{SS_{res}}{SS_{tot}}\]

\[SS_{tot} = \sum^n_{i=1} (y_i- \hat{y_i})^2\]

\[SS_{res} = \sum^n_{i=1} (y_i - \bar{y})^2\]

\[ \bar{y} = \frac{1}{n} \sum^n_{i=1} y\]

% \justify
Para essa métrica, o modelo pode performar arbitrariamente mal, com esse valor
podendo se tornar arbitrariamente negativo. Porém, seu valor máximo é 1,
indicando um modelo ideal.\\


\subsection{Erro Quadrático Médio}
\label{sec:MSE}
O erro quadrático médio (em inglês, MSE) é uma medida de diferença entre duas variáveis contínuas
X e Y \citep{cohen}. Ele é dado por: \\

\[MSE = \sum^n_{i=1}\frac{(y_i - \hat{y_i})^2}{n}\]

Essa métrica é uma medida absoluta. E quanto maior esse valor pior o modelo está performando.



%% ------------------------------------------------------------------------- %%

\section{Sequencialidade dos Dados}

Em um problema canônico de aprendizado supervisionado gostariamos de aprender
uma função $f$ tal que para um par inédito de dados $x^*,y^*$, a nossa função
dependa apenas de $x^*$ para gerar uma predição. Para os dados da Intercement pode ser que um valor i.e. índice de dureza dependa não só da última entrada, mas de diversas entradas anteriores. 

Ou seja, nossos dados podem ter sido gerados por uma distribuição de
probabilidade da forma $p(y | x_{t} ,x_{t -1},x_{t -2},x_{t-3} , \dots,
x_{t-T})$, onde uma saida $y$ é condicionada pelas últimas $T$ entradas. Para
resolver um problema de aprendizado dessa natureza, devemos usar \textbf{modelos
  sequenciais} \citep{dlbook}. 

Iremos então experimentar com modelos sequenciais e não-sequencias para testar a acurária de ambos no problema em questão.


\section{Modelos Usados} 

\subsection{Redes Neurais}


Redes neurais são aproximadores universais de funções \citep{nnuni}. Dado um problema
de classificação onde se deseja aprender uma função da forma $y = f^*(x)$, uma
Rede Neural define um mapeamento $y = f(x ; \theta)$, onde $\theta$ é o vetor de
parâmetros que serão aprendidos com o fim de minimizar a diferença entre a
distribuição empírica e essa distribuição gerada pelo modelo. Essa diferença pode
ser minimizada pelo método da verossimilhança, assim como elaborado na sessão~\ref{sec:reglog}.\\

O modelo neural é uma composição de funções que unem uma transformação linear e
a aplicação de uma função não-linear $\sigma$: \\

\[ f(x)=  \sigma(W*x + b) \]

Importante ressaltar que $W$ é uma matriz, assim como $x$ e $b$ são vetores. \\

A computação da saída de uma rede neural então pode ser escrita como:

\[   y = f_n \circ f_{n-1} \circ f_{n-2} \dots f_1(x)  \]

Para uma rede neural de $n$ \textbf{camadas}. Onde cada camada será uma função
$f_i$ cujos parâmetros são $W_i$ e $b_i$. Portanto, para a i-ésima camada da rede
sua saída sera da forma: 

\[ f_i (x)=  a_i = \sigma(W_i*a_{i-1} + b_i) \]

Onde $a_{i-1}$ é a saída da camada anterior, também chamada de
\textbf{ativação}. A saída dessa camada é então a sua ativação $a_i$. \\ 

Vale notar que uma saída $y$ calculada por uma rede neural depende unicamente dos
seus parâmetros e da entrada $x$. Isso não será verdade para os modelos
\textbf{sequenciais} que também serão usados nesse trabalho, onde o estado
interno de computação desses modelos é usado como entrada para uma próxima
iteração \citep{dlbook}. \\

A seguir está reproduzida uma rede neural simples com uma camada oculta e dois neurônios de saída.


%\input{tiks/NN.tex}


Na figura mostramos como seria uma rede que usa como parâmetros alguns dos dados de Farinha para modelar os índices RC3 e RC7 dos dados de expedição.

\bigskip


\subsection{Inferência Bayesiana em Machine Learning}

O tratamento Bayesiano para modelos de ML é bastante diverso dos frequentistas \citep{dlbook}.
Em uma análise frequentista estima-se um valor de $\theta$ e então todas as
predições são feitas a partir desse valor. No caso Bayesiano se consideram todos
os possíveis valores de $\theta$ ao se fazer uma predição. É preciso especificar
um grau de certeza \textbf{a priori} $p(\theta)$ sobre os parâmetros, e então
consideramos que os dados \textbf{foram observados} e usamos a lei de Bayes para
calcular a \textbf{probabilidade} posterior $p(\theta | X,Y)$ usando para tal
$p(X,Y | \theta)$, a chamada verossimilhança \citep{bayesml}. 

\[    p(\theta | X,Y) = \frac{p(Y| X,\theta) p(\theta)}{p(Y)}   \]

Finalmente, para realizar uma inferência devemos integrar por toda a distribuição $p(\theta)$ marginalizando esse parâmetro. Se por exemplo queremos uma nova anotação $y^*$ para um novo dado $x^*$:

\[ p(y^* | x^* , X,Y) = \int  p(y^* | x^*,\theta) p(\theta | X,Y)  d\theta \]

Essa integral é geralmente intratável pela dificuldade de se calcular
analiticamente $p(\theta | X,Y)$ \citep{ubertime}, e portanto será usada uma
técnica que aproxima uma inferência Bayesiana numericamente em redes neurais, o Monte Carlo
Dropout \citep{dropbayes}. \\



O MC Dropout consiste no uso de \textbf{Dropout} em todas as camadas da rede
neural, i.e. descartar ativações aleatóriamente entre duas camadas da rede com
probabilidade $p$.\\

\begin{figure}
  \input{chapters/dropout.tex}
  \label{fig:dropout}
  \caption{Representacão do uso de Dropout em uma rede neural}
\end{figure}

Formalmente, para um vetor de ativações $a$ na saída de uma
cada da rede neural, amostramos um vetor de \textit{máscara} $r$ de uma
distribuição de Bernoulli cujo parâmetro é $p$, i.e. $r \sim Bernoulli(p)$.
Finalmente, o novo vetor de ativações é calculado por $r * a$. O efeito desse
processo é de zerar aleatoriamente entradas do vetor de ativação.
\\ 

\cite{dropbayes} demonstram que a aplicação do dropout em todas as camadas do
modelo equivale em aproximar numéricamente uma distribuição $q(\theta)$ que
cumpriria o papel da distribuição intratável $p(\theta | X,Y)$ na inferência Bayesiana. \\

Para o cálculo da incerteza das predições do modelo se realizam então $B$ computações estocásticas de uma mesma
saída $y^*$, sejam elas $Y^* = \{y^*_{(1)},y^*_{(2)}, \dots , y^*_{(B)}\}$. A incerteza
  dessa medida é calculada então pela variância amostral de $Y^*$.



\subsection{Random Forest}

Random Forests são um método de \textbf{Ensemble Learning} para classificação ou regressão. \textbf{Ensemble Learning} é uma família de técnicas no qual diversos modelos "fracos" são usados em conjunto com algum sistema de votação para que a a acurária do sistema em conjunto se torne melhor que a de qualquer um dos modelos sozinho. Seguindo essa ideia, Random Forests são conjuntos de diveras árvores de decisão simples unidas por um meta-algoritmo de votação para que se produza uma predição muito mais eficaz.


\section{Modelos Sequenciais}
Com o sucesso de modelos sequenciais no campo do Deep Learning, iremos averiguar se é possível modelar sequencialmente os dados da produção de cimento usando dois desses modelos. 
\\

\subsection{Rede Neural Recorrente}
% \input{tiks/RNNSimplified.tex}

A família das Redes Neurais Recorrentes é composta por modelos especializados
em processar dados sequenciais \citep{dlbook}, da forma $x^{(1)},x^{(2)} ,x^{(3)}\dots ,x^{(T)}$. Uma rede neural recorrente é definida por uma função com
\textbf{recorrência} ou \textbf{recursão}, de modo que no processamento de uma
sequência o estado da rede seja de certo modo propagado temporalmente. A equação
a seguir ilustra uma função com recorrência: \\

\[h^{(t)} = f(h^{(t-1)},x^{(t)};\theta)\]

Nessa equação notamos que na iteração $t$ o valor do vetor $h$ depende de
$h_{t-1}$.RNNs aprendem a usar esse vetores como esse como
\say{resumos} das iterações passadas. Desse modo , o modelo ganha a capacidade
de usar informações passadas da sequência para o calculo
de uma saída. Essa capacidade porém começa a ser dificultada quando o modelo
realiza muitas iterações temporais. Durante o aprendizado, sequências muito
extensas criam problemas numéricos para o cálculo dos gradientes, esses podendo
tomar valores muito pequenos ou muito grandes. Foram propostos outros modelos
que não possuam esse problema, como o modelo LSTM, explicado na sessão~\ref{sec:lstm}. 

%%%
\begin{figure}[H]
\centering
\includegraphics[width=0.9\columnwidth]{rnn.png}
\caption{Grafo de Computação de uma RNN genérica \citep{dlbook}}
\label{fig:rnngraph}
\end{figure}
%%%

Como podemos ver na Figura~\ref{fig:rnngraph}, a entrada $x$, ao lado do estado
interno $h$, são usados para calcular um novo estado. Nota-se que o vetor $h$,
bem como os parâmetros internos da RNN, são divididos entre iterações temporais.
\\

Essa classe de modelos normalmente é usada para modelagem de linguagem. Buscando
estimar uma distribuição de probabilidade $p(w_t | w_{t-1},w_{t-2},w_{t-3} \dots
) $ onde os $w_i$ são palavras subsequentes de um texto. Normalmente um modelo
dessa natureza busca resolver um problema de classifição, onde a próxima palavra
a ser prevista pelo modelo é uma entre todas as possibilidades de um certo
vocabulário. No caso do domínio em questão desejamamos resolver um problema de
regressão, onde nosso alvo é um valor numérico. Para treinar um desses modelos,
precisamos usar como entrada exemplos subsequentes de dados, onde cada exemplo
de entrada tem um exemplo pareado de saída. Basicamente redes neurais
recorrentes funcionam recebendo um exemplo de entrada, criando uma representação
interna com o mesmo e então gerando uma saída e comparando essa saída com o
exemplo de saída real, gerando um erro. Finalmente, esse erro é propagado para
alterar seus parâmetros (com o fim de achar um conjunto de parâmetros que gere
boas previsões). \\ 


Como já explicado anteriormente, nossos dados de entrada e saída não estão necessariamente pareados perfeitamente dia a dia. Portanto, foi necessário achar intervalos de tempo nos dados onde existe esse pareamento. Isso reduz drasticamente quais períodos representados nos dados realmente podem ser usados para treinar um desses modelos.


\subsubsection{LSTM}
\label{sec:lstm}

LSTMs \citep{lstm} são um tipo de RNN que por meio de sua arquitetura permitem que sequências
maiores sejam processadas sem que o fluxo dos gradientes propagados pela rede se torne
numericamente problemático (i.e. tendendo a 0 ou a infinito). A Figura~\ref{fig:lstm} ilustra o fluxo dos sinais em uma LSTM. \\
\begin{figure}
\centering
\caption{Diagrama da arquitetura de uma LSTM}
\input{chapters/lstm.tex}
\label{fig:lstm}
\end{figure}


Uma rede LSTM possui três \say{portas}. Cada porta possui duas matrizes $W,U$ e um
vetor $b$ de parâmetros. Uma iteração da LSTM começa com o cálculo dos sinais
$o_t,i_t,f_t$.\\

\[f_t = \sigma_g(W_fx_t + U_fh_{t-1} + b_f)\]
\[i_t = \sigma_g(W_ix_t + U_ih_{t-1} + b_i)\]
\[o_t = \sigma_g(W_ox_t + U_oh_{t-1} + b_o)\]

O diferencial de uma LSTM é a propagação do sinal $c_t$, a célula de memória.
Esse valor depende de $f_t$ e $i_t$, que influenciam em como o valor da
célula de memória será atualizado na presente iteração. A equação a seguir
mostra como o valor da célula de memória é calculado. Onde $\circ$ é o produto Hadamard, ou apenas multiplição entrada por entrada de
duas matrizes ou vetores. \\

\[c_t = f_t \circ c_{t-1} + i_t \circ \sigma_c(W_cx_t + U_ch_{t-1} + b_c)\]

Nota-se que $f_t$
define quando de valor antigo da célula de memória deve participar no cálculo do
seu novo valor. 
Da mesma maneira $i_t$ define quanto da nova entrada deve ser usada no cálculo desse valor.
Em outras palavras, as portas $i_t$ e $f_t$ definem o quanto a LSTM deve,
respectivamente, \say{lembrar} e \say{esquecer}.


O novo estado da LSTM é então calculado por: \\
\[h_t = o_t \circ \sigma_h(c_t)\]


\subsubsection{Rede Encoder-Decoder}

Redes Encoder-Decoder são usadas para modelagem de sequência para
sequência, ou seja, para receber dados sequênciais como entrada e gerar
sequências como saída \citep{dlbook}. Esses modelos possuem duas partes, ambas compostas por
RNNs. \\

\begin{figure}[H]
\centering
\input{chapters/encdec.tex}
\caption{ Diagrama de Rede Encoder-Decoder.\\ Modificado de \cite{encdec}}

\end{figure}
  
O \textbf{encoder} é uma RNN que busca receber uma sequência de entrada de
tamanho arbitrário e gerar uma representação como saída, o seu estado interno,
representando em marrom no diagrama acima. O \textbf{decoder} então recebe essa representação interna (também chamada
de \textit{embedding}) e usa-a para gerar saídas sequencialmente, podendo
então gerar essas sequências de duas formas: O decoder pode usar as suas proprias saídas
como entrada para gerar a saída da próxima iteração temporal, ou então usar dados de treinamento como
entradas, essa última forma chamada de \textit{teacher forcing}. No diagrama
está indicado o embedding entre o decoder e o encoder. Esse embedding é apenas
um vetor (cuja dimensão é um hiper-parâmetro de treinamento) que resume a
informação sequencial lida pelo encoder e a transmite para o decoder.\\

Redes encoder-decoder são muito usadas para aprender \textit{representações} para os
dados. Isso quer dizer que os embeddings da rede encoder-decoder são frutos de
transformações no espaço das entradas que buscam extrair a informação mais útil
para o objetivo do aprendizado. Podemos pensar nessas transformações como
mudanças de coordenadas nas quais estamos representando os dados. A
imagem~\ref{} ilustra essa ideia. \\



\subsubsection{Modelo Encoder-Decoder-Forecaster}

Iremos reproduzir nesse trabalho o modelo proposto em \cite{ubertime}. A
arquitetura consiste em uma rede \textbf{encoder-decoder} que aprende embeddings da
série temporal e então uma rede \textbf{forecaster} que usa esses embeddings ao lado de
variáveis exógenas a série temporal para realizar predições.  


\begin{figure}[H]
\centering
\includegraphics[width=0.9\columnwidth]{uber.png}
\caption{Arquitetura do modelo proposto por \cite{ubertime}}
\end{figure}


Durante o pré-treinamento a rede encoder-decoder consome sequências de $F$ dias
da série temporal. O encoder cria uma representação vetorial $h$ depois de
receber como entrada os primeiros $T$ ($T < F$) dias da sequência. Então, $h$ é usado como
inicialização do estado interno do decoder, e esse então consume mais $F - T$
entradas da sequência. Para o decoder, se em uma iteração sua entrada é $X_i$,
então sua saída será comparada com $X_{i+1}$, e esse erro é propagado após lidas
todas as entradas para que a representação $h$ proveniente do encoder possa se
tornar mais informativa.


Esse modelo também possui outra característica importante. Todas as camadas da
redes neurais que compõe o encoder, o decoder e o forecaster possuem
Dropout com probabilidade $p$. Ou seja, podemos usar a técnica do Monte
Carlo Dropout para estimar a variância de cada predição feita por esse modelo. \\










%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../quali"
%%% bibtex-file-path: "../bibliografia"
%%% End: