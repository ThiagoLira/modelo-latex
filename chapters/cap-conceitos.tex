%% ------------------------------------------------------------------------- %%
\chapter{Conceitos}
\label{cap:conceitos}


\section{Definição do Problema}


Os dados da produção de cimento são anotados temporalmente, então podemos escolher
tratá-los como uma série temporal, i.e. a ordem de entrada dos dados no modelo
é uma informação relevante. A tarefa de regressão se torna então prever as próximas anotações desses
dados num horizonte de tempo futuro finito. A série temporal é composta por diversas medições indexadas pelo tempo $t$ do objetivo
de predição $y$ e de um vetor $\textbf{x}$ de parâmetros. O problema é de Aprendizado
Supervisionado: Buscamos a distribuição das anotações $y$ condicionada
aos parâmetros de entrada $x$, mas também ao
histórico da série temporal, i.e. valores passados do objetivo $y$ e do
vetor de entrada $\textbf{x}$. 
Seja um série temporal alvo, $y$, e diversas séries temporais de variáveis
 $x_i$ com valores nos instantes de tempo no intervalo $[1,T]$. Seja $T$ o último dia em que se possui dados de todas essas
séries. Desejamos modelar a distribuição de probabilidade das próximas anotações
de $y$ até uma nova data $F + T$, $F \in \mathbb{N}$, condicionadas por uma janela de
observações das diversas variáveis  $x_i$ e do alvo $y$ no passado \citep{deepfactors}.

\[ p(y_{T:F} | y_{1:T},\textbf{x}_{{1}:T}) \]

A distribuição será aprendida por meio de aprendizado supervisionado, onde os
modelos serão alimentados com diversos pares de entradas,
$(y_t,\textbf{x}_{t:t+W})$, onde $\textbf{x}$ é um vetor que contém os valores de
cada variável independente $x_i$, no instante de tempo $t$. Cada tupla de
entrada contém $W$ entradas do vetor de entrada. O parâmetro $W$, o
tamanho da janela de tempo consumida para gerar cada saída, é escolhido antes do
treino e pode ser alterado na busca de uma melhor acurácia de predição.

\section{Estatística Frequentista e Estatística Bayesiana}
 
Como nesse trabalho serão usados métodos de inferência Bayesiana aplicados a Aprendizado Automático,
cabe então uma breve elaboração das diferenças entre as duas principais
vertentes da estatística \citep{dlbook}. 

Digamos que exista um evento aleatório que tenha um resultado com probabilidade
$p$ de acontecer. A visão frequentista de probabilidades diz que, se pudéssemos
repetir infinitas vezes esse evento, a proporção de vezes que esse resultado irá
acontecer se aproximará arbitrariamente de $p$. E então entenderíamos a
probabilidade $p$ meramente como uma proporção de resultados positivos em uma
certa amostra de experimentos. Mas e se o evento não pudesse ser repetido?
Quando físicos criam modelos para explicar o nascimento do universo, é
impossível pensar em incerteza como repetições de fenômenos cosmológicos como esse. Nesse segundo caso, resultados são derivados de \textbf{graus de certeza}, onde a chance de um evento acontecer é estimada pela aplicação de conhecimentos prévios em vista de algo que foi observado posteriormente. A primeira maneira de se entender estatística é chamada de Frequentista e a segunda de Bayesiana. \\

E no campo de Aprendizado Automático, as duas maneiras de se gerar predições são estimadores
frequentistas (Seção~\ref{sec:est}) e inferência Bayesiana (seção~\ref{sec:bayesinf}).



\section{Aprendizado Automático}

O campo de Aprendizado Automático (Machine Learning - Aprendizado Automático) é um ramo da Ciência
da Computação que utiliza métodos estatísticos para criar sistemas que possam
aprender a realizar uma determinada tarefa através de dados. Um problema de Aprendizado Automático possui
as seguintes partes \citep{dlbook}: \\

\begin{itemize}

  
\item A tarefa $T$, no caso dos dados da produção de cimento, um problema de regressão.
  Onde desejamos estimar valores numéricos a partir de uma matriz de dados $X$ e
  suas anotações numéricas $Y$. 
  
\item Uma métrica de performance $P$, que define a distância entre as predições
  do modelo e os dados de treinamento. A teoria por trás desses métricas será explicada na seção~\ref{sec:metricas}.

\item A experiência $E$, que define qual tipo de informação o modelo poderá usar
  durante o treinamento. Explicaremos esse conceito na seção~\ref{sec:exp} 
  
\end{itemize}

\subsection{A Experiência}
\label{sec:exp}
Algoritmos de Aprendizado Automático podem ser divididos nas categorias de
aprendizado supervisionado, não supervisionado e aprendizado por reforço
\citep{dlbook}. Técnicas dos dois primeiros tipos serão usadas para os dados de
produção de cimento.


\subsubsection{Aprendizado Supervisionado}
\label{sec:apren} 
Aprendizado Supervisionado consiste, do ponto de vista estatístico, em aprender uma distribuição
de probabilidade condicional do tipo $p(y | x)$. Nos são fornecidos diversos vetores de
entrada $x$ associados a uma anotação $y$ e gostaríamos de modelar essa relação
entre dados e anotações para que possamos anotar novos dados inéditos para os
modelos. Ou seja, para diversos exemplos de vetores
$x$ são fornecidas anotações $y$, e desejamos então criar predições de anotações
$y'$ para novas entradas $x'$.


\subsubsection{Aprendizado Não Supervisionado}

Para o caso de Aprendizado Não Supervisionado, mantendo a estrutura do exemplo
anterior, desejaríamos então modelar uma distribuição do tipo $p(x)$, onde temos
também diversos exemplos de vetores aleatórios $x$ e estamos estudando alguma propriedade importante dessa distribuição.



\subsection{A Performance}
\label{sec:metricas}

\subsubsection{Estimadores}
\label{sec:est}

A aprendizagem estatística busca a diminuição de \textit{risco empírico} das predições feitas por
um modelo \citep{mlprob}. O risco empírico é dado pelo somatório de uma função
de custo por todo o conjunto de dados:

\[
  \mathcal{L} = \frac{1}{T}\sum_{t=0}^{i=T}L(y_t,\hat{y_t}) 
\]

Um método possível da diminuição de risco é o de guiar o aprendizado pela
maximização da verossimilhança do modelo. Esse valor é a probabilidade de termos
observado os dados alvo condicionada pelos parâmetros do modelo e as entradas
(que iremos representar pelo vetor $\theta$). Uma iteração de
treino então consistira do produtório de todas as verossimilhanças para cada par
de entradas. Na definição do nosso problema, temos que o custo total do modelo
$\mathcal{L}$ é dado pela seguinte equação \citep{dlbook}, considerando que o
modelo recebe $W$ entradas temporais para o calculo da verossimilhança de uma saída:

\[
\mathcal{L} = \prod_{t=0}^{i=T} p(y_t | \textbf{x}_{t}, \theta) 
\]

É comum, porém, por motivos de estabilidade numérica \citep{dlbook}, substituir
o produtório de probabilidades pelo somatório do logarítimo das probabilidades: 

\[
  \mathcal{L} = \sum_{t=0}^{i=T} \log{p(y_t | \textbf{x}_{t},\theta)}
\]

Em uma análise bayesiana, também buscaríamos maximizar a verossimilhança dos
dados condicionados aos parâmetros do modelo. Mas como exige a formalização
bayesiana, iriamos considerar uma distribuição a priori dos parâmetros do
modelo, i.e. $p(\theta)$:


\[
  \mathcal{L} = \prod_{t=0}^{i=T}p(y_t|\textbf{x}_{t},\theta)p(\theta) 
\]


Para modelos com baixa dimensionalidade e muitos dados, a estimativa bayesiana
coincide com a frequentista, convergindo para a \textbf{estimação de máxima
verossimilhança} \citep{mlprob}. Isso se deve ao fato que a distribuição de
probabilidade a priori progressivamente se torna menos importante no calculo da
probabilidade posterior com muitos dados. Por isso se diz que a Inferência
Bayesiana é um estimador consistente \citep{mlprob}, i.e. ele converge para uma
hipótese arbitrariamente próxima da verdade.


Na seção~\ref{sec:reglog} iremos dar um exemplo dessa cálculo de verossimilhança
assumindo que o erro do modelo é distribuído normalmente.

\subsubsection{A Distribuicão Normal}
\label{sec:reglog}

Uma variável aleatória Y distribuída normalmente possui uma função densidade de
probabilidade da seguinte forma \citep{mlprob}:

\begin{equation}
  \label{eq:ver}
p(y| \mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp{-\frac{(y - \mu)^2}{2\sigma^2}}
\end{equation}

Esse valor também é chamado de verossimilhança (a probabilidade de termos
observado um valor de Y, dado que essa variável é distribuída normalmente, com média $\mu$ e variância $\sigma^2$), e
normalmente trabalha-se com o logaritmo desse valor, para podermos trocar um
produtório por somatório para agregar os erros gaussianos de um conjunto de dados
i.i.d.. Nesse caso, temos pares de predição e valor real, que assumirá o papel
da média $\mu$. A verossimilhança total de um conjunto de diversos pares
$(\hat{y},y)$ é dado por: 

\begin{equation}
\label{eq:logver1}
  \mathcal{L}= -m \log \sigma - \frac{m}{2} \log (2 \pi) - \sum^m_{i=1}
  \frac{\abs{\hat{y_i}-y_i}^2}{2 \sigma^2}
\end{equation}

Um estimador sem viés da Equação~\ref{eq:logver1}, chamado de Erro Quadrático
Médio, pode ser escrito apenas como \citep{mlprob}:

\begin{equation}
  \label{eq:logver2}
  \mathcal{L}=  \sum^m_{i=1} (\hat{y_i}-y_i)^2
\end{equation}


\subsubsection{Função de Custo}

A Função de Custo é uma função usada para guiar os algoritmos de otimização na
direção de parâmetros que melhorem a performance do modelo \citep{dlbook}.
Essa função para alguns modelos pode representar a verossimilhança dos dados,
mas isso não é necessário no geral.

Nessa sessão iremos definir a função de custo usada em todos os modelos de regressão usados. \\

\subsubsection{Erro Quadrático Médio}
\label{sec:MSE}

O Erro Quadrático Médio é um estimador não enviesado da Equação~\ref{eq:ver}
\citep{dlbook} para o cálculo da verossimilhança de um modelo Gaussiano. Ele é calculado em relação a duas variáveis contínuas
$(\hat{y},y)$, definido da seguinte forma:

\[MSE = \sum^n_{i=1}\frac{(\hat{y_i} - y_i)^2}{n}\]

Essa métrica é uma medida absoluta. E quanto maior esse valor pior o desempenho
do modelo, i.e. mais distante suas predições estão da média real. \\

\subsubsection{Divergência KL}

A divergência KL, ou Kullback-Leibler, é uma medida de diferença entre duas distribuições de probabilidade (também chamada de Entropia Relativa). \\
Essa medida pode ser definida pelo valor esperado da diferença logarítmica da probabilidade dos dados serem observados em uma distribuição ou na outra \citep{dlbook}. \\
Sejam duas distribuições de probabilidade $p$ e $q$, bem como uma matriz de dados $X$: \\

\[D_{KL}(p || q) = \mathop{\mathbb{E}}[\log p(x) - \log q(x)]​\]

Ou em uma forma mais usual: \\

\[D_{KL}(p||q) = \sum_{i=1}^{N}p(x_{i}) \log \frac{p(x_{i})}{q(x_{i})}​\]



\subsection{Métrica de Acurácia}

Diferentemente da função de custo, a métrica de acurácia não é usada para guiar a otimização do modelo,
mas para testar as predições frente a realidade e avaliar sua qualidade. \\

\subsubsection{R-quadrado}
Como teste da acurácia dos modelos foi usada a métrica R-quadrado ($R^2$) \citep{cohen}. Sejam $\hat{y}$ e $y$ nossa previsão dada pelo modelo e o seu valor real, a acurácia do modelo é dada por:\\

\begin{align}
&R^2 = 1 - \frac{SS_{res}}{SS_{tot}} &\\
&SS_{tot} = \sum^n_{i=1} (y_i- \hat{y_i})^2 &\\
&SS_{res} = \sum^n_{i=1} (y_i - \bar{y})^2 &\\
&\bar{y} = \frac{1}{n} \sum^n_{i=1} y &
\end{align}

% \justify
$SS_{res}$ mensura a quantidade de variância \textit{residual}, a
variância entre os dados reais e as predições do modelo, i.e. a variância não
explicada pelo modelo. $SS_{tot}$ cumpre o papel da variância \textit{total}. A métrica $R^2$
então nos mostra quanto da variância dos dados é explicada pelo modelo. \\

Para essa métrica, o modelo pode ter um desempenho arbitrariamente ruim, com esse valor
podendo se tornar arbitrariamente negativo. Porém, seu valor máximo é 1,
indicando um modelo ideal.\\


\subsubsection{Custo quantílico}

O custo quantílico \citep{deepar} é usado na avaliação de performance de
predições temporais. \\

Para uma predição $y^*$ com desvio-padrão $\sigma$, um quantil $q$ representa o
valor para o qual uma fração $q$ das predições está fora do grupo.

Por exemplo, usando o quantil $q=0.9=90\%$, assumindo uma distribuição normal,
podemos calcular exatamente a quantos desvios-padrões estão $10\%$ dos valores
presentes na distribuição. Para $q=0.9$, uma tabela z nos mostra que devemos
usar $1.29$ desvios-padrões:

\[
  y^*(\rho = 0.9) = y^* + 1.29\sigma
\]


Dado um quantil $p \in (0,1)$ um valor real $y_{t}$ e uma predição
$y^{*}_{t}(\rho)$, define-se o custo quantílico por: \\

\begin{equation}
  \mathcal{QL}_{\rho}(y_{t},y^{*}_{t}(\rho)) =
\begin{cases*}
  2 \rho(y_{t} - y^{*}_{t}(\rho)) & se $ y_{t} - y^{*}_{t}(\rho) > 0$ \\
  2 (1 - \rho)(y^{*}_{t}(\rho) - y_{t}) & se $y_{t} - y^{*}_{t}(\rho) \leq 0$
\end{cases*}
\end{equation}


Esse valor é bastante informativo quando a predição toma a forma de uma faixa de
valores (como uma distribuição de probabilidade) ao invés de apenas um valor
pontual, como é comum para problemas de regressão. Ele nos informa o risco
ponderado de um certo quantil da predição. 

%% ------------------------------------------------------------------------- %%


\section{Modelos Clássicos} 


\subsection{Redes Neurais}

\label{sec:nn}
Redes neurais são aproximadores universais de funções \citep{nnuni}. Dado um problema
de classificação onde se deseja aprender uma função da forma $y = f^*(x)$, uma
Rede Neural define um mapeamento $y = f(x ; \theta)$, onde $\theta$ é o vetor de
parâmetros que serão aprendidos com o fim de minimizar a diferença entre a
distribuição empírica e essa distribuição gerada pelo modelo. Essa diferença pode
ser minimizada pelo método da verossimilhança, assim como elaborado na sessão~\ref{sec:reglog}.\\

O modelo neural é uma composição de funções que unem uma transformação linear e
a aplicação de uma função não-linear $\sigma$: \\

\[ f(x)=  \sigma(W*x + b) \]

Importante ressaltar que $W$ é uma matriz, assim como $x$ e $b$ são vetores. \\

A computação da saída de uma rede neural então pode ser escrita como:

\[   y = f_n \circ f_{n-1} \circ f_{n-2} \dots f_1(x)  \]

Para uma rede neural de $n$ \textbf{camadas}, onde cada camada será uma função
$f_i$ cujos parâmetros são $W_i$ e $b_i$. Portanto, para a i-ésima camada da rede
sua saída será da forma: 

\[ f_i (x)=  a_i = \sigma(W_i*a_{i-1} + b_i) \]

Onde $a_{i-1}$ é a saída da camada anterior, também chamada de
\textbf{ativação}. A saída dessa camada é então a sua ativação $a_i$. \\ 

Vale notar que uma saída $y$ calculada por uma rede neural depende unicamente dos
seus parâmetros e da entrada $x$. Isso não será verdade para os modelos
\textbf{sequenciais} que também serão usados nesse trabalho, onde o estado
interno de computação desses modelos é usado como entrada para uma próxima
iteração \citep{dlbook}. \\

Na Figura~\ref{fig:nn} mostramos como seria uma rede que usa os índices RC3 e RC7 para
modelar como saída o índice RC28. \\  

\begin{figure}
  \centering
  \input{chapters/NN.tex}
  \caption{Um exemplo de Rede Neural no contexto dos dados de produção de cimento}
  \label{fig:nn}
\end{figure}


\bigskip

\subsubsection{Regularização L2}
 
\subsection{Random Forest}

Random Forests são um método de \textbf{Ensemble Learning} para classificação ou regressão. \textbf{Ensemble Learning} é uma família de técnicas no qual diversos modelos \say{fracos} são usados em conjunto com algum sistema de votação para que a a acurácia do sistema em conjunto se torne melhor que a de qualquer um dos modelos sozinho. Seguindo essa ideia, Random Forests são conjuntos de diversas árvores de decisão simples unidas por um meta-algoritmo de votação para que se produza uma predição muito mais eficaz.


\section{Modelos Sequenciais}

Modelos sequenciais consomem sequências de dados e também podem ser usados pra
criar predições sequenciais \citep{dlbook}. A área de NLP viu muitos novos
resultados com o uso esses tipos de modelos para geração e classificação de textos, que são dados com
natureza inerentemente sequencial. Para o caso desse trabalho, séries temporais também podem
aproveitar da capacidade desses modelos de lerem multiplas entradas ao mesmo tempo.

\subsection{Rede Neural Recorrente}
% \input{tiks/RNNSimplified.tex}

A família das Redes Neurais Recorrentes (RNN) é composta por modelos especializados
em processar dados sequenciais \citep{dlbook}, da forma $x^{(1)},x^{(2)} ,x^{(3)}\dots ,x^{(T)}$. Uma rede neural recorrente é definida por uma função com
\textbf{recorrência} ou \textbf{recursão}, de modo que no processamento de uma
sequência o estado da rede seja de certo modo propagado temporalmente. A equação
a seguir ilustra uma função com recorrência: \\

\[h^{(t)} = f(h^{(t-1)},x^{(t)};\theta)\]

Nessa equação notamos que na iteração $t$ o valor do vetor $h$ depende de
$h_{t-1}$.RNNs aprendem a usar esse vetores como
\say{resumos} das iterações passadas. Desse modo , o modelo ganha a capacidade
de usar informações passadas da sequência para o cálculo
de uma saída. Essa capacidade porém começa a ser dificultada quando o modelo
realiza muitas iterações temporais. Durante o aprendizado, sequências muito
extensas criam problemas numéricos para o cálculo dos gradientes, esses podendo
tomar valores muito pequenos ou muito grandes, esse fenômeno recebe o nome de
\textbf{gradientes desaparecidos}. Foram propostos outros modelos
que não possuam esse problema, como o modelo LSTM, explicado na sessão~\ref{sec:lstm}. 

%%%
\begin{figure}[H]
\centering
\includegraphics[width=0.9\columnwidth]{rnn.png}
\caption{Grafo de Computação de uma RNN genérica \citep{dlbook}}
\label{fig:rnngraph}
\end{figure}
%%%

Como podemos ver na Figura~\ref{fig:rnngraph}, a entrada $x$, ao lado do estado
interno $h$, são usados para calcular um novo estado. Nota-se que o vetor $h$,
bem como os parâmetros internos da RNN, são divididos entre iterações temporais.
\\

Essa classe de modelos normalmente é usada para modelagem de linguagem. Buscando
estimar uma distribuição de probabilidade $p(w_t | w_{t-1},w_{t-2},w_{t-3} \dots
) $ onde os $w_i$ são palavras subsequentes de um texto. Normalmente um modelo
dessa natureza busca resolver um problema de classificação, onde a próxima palavra
a ser prevista pelo modelo é uma entre todas as possibilidades de um certo
vocabulário. No caso do domínio em questão desejamos resolver um problema de
regressão, onde nosso alvo é um valor numérico. Para treinar um desses modelos,
precisamos usar como entrada exemplos subsequentes de dados, onde cada exemplo
de entrada tem um exemplo pareado de saída. Basicamente redes neurais
recorrentes funcionam recebendo um exemplo de entrada, criando uma representação
interna com o mesmo e então gerando uma saída e comparando essa saída com o
exemplo de saída real, gerando um erro. Finalmente, esse erro é propagado para
alterar seus parâmetros (com o fim de achar um conjunto de parâmetros que gere
boas previsões). \\ 


Como já explicado anteriormente, nossos dados de entrada e saída não estão necessariamente pareados perfeitamente dia a dia. Portanto, foi necessário achar intervalos de tempo nos dados onde existe esse pareamento. Isso reduz drasticamente quais períodos representados nos dados realmente podem ser usados para treinar um desses modelos.


\subsubsection{LSTM}
\label{sec:lstm}

LSTMs \citep{lstm} são um tipo de RNN que por meio de sua arquitetura permitem que sequências
maiores sejam processadas sem que o fluxo dos gradientes propagados pela rede se torne
numericamente problemático (i.e. tendendo a 0 ou a infinito). A
Figura~\ref{fig:lstm} ilustra o fluxo dos sinais em uma LSTM. Observa-se que a
entrada $x_t$ e o antigo estado $h_{t-1}$ são usados para o cálculo de diversos \textit{gates} ou \say{portas}, e
finalmente um novo estado $h_t$ é gerado.\\

\begin{figure}
\centering
\caption{Diagrama da arquitetura de uma LSTM}
\input{chapters/lstm.tex}
\label{fig:lstm}
\end{figure}




Uma rede LSTM possui três \say{portas}. Cada porta possui duas matrizes $W,U$ e um
vetor $b$ de parâmetros. Uma iteração da LSTM começa com o cálculo dos sinais
$o_t,i_t,f_t$.\\

\[f_t = \sigma_g(W_fx_t + U_fh_{t-1} + b_f)\]
\[i_t = \sigma_g(W_ix_t + U_ih_{t-1} + b_i)\]
\[o_t = \sigma_g(W_ox_t + U_oh_{t-1} + b_o)\]

O diferencial de uma LSTM é a propagação do sinal $c_t$, a célula de memória.
Esse valor depende de $f_t$ e $i_t$, que influenciam em como o valor da
célula de memória será atualizado na presente iteração. A equação a seguir
mostra como o valor da célula de memória é calculado. Onde $\circ$ é o produto Hadamard, ou apenas multiplicação entrada por entrada de
duas matrizes ou vetores. \\

\[c_t = f_t \circ c_{t-1} + i_t \circ \sigma_c(W_cx_t + U_ch_{t-1} + b_c)\]

Nota-se que $f_t$
define quanto do valor antigo da célula de memória deve participar no cálculo do
seu novo valor. 
Da mesma maneira $i_t$ define quanto da nova entrada deve ser usada no cálculo desse valor.
Em outras palavras, as portas $i_t$ e $f_t$ definem o quanto a LSTM deve,
respectivamente, \say{lembrar} e \say{esquecer}.


O novo estado da LSTM é então calculado por: \\
\[h_t = o_t \circ \sigma_h(c_t)\]




\subsubsection{Rede Encoder-Decoder}
\label{sec:encdec}
Redes Encoder-Decoder são usadas para modelagem de sequência para
sequência, ou seja, para receber dados sequenciais como entrada e gerar
sequências como saída \citep{dlbook}. Esses modelos possuem duas partes, ambas compostas por
RNNs. \\

\begin{figure}[H]
\centering
\input{chapters/encdec.tex}
\caption{ Diagrama de Rede Encoder-Decoder.\\ Modificado de \citep{encdec}}

\end{figure}
  
O \textbf{encoder} é uma RNN que busca receber uma sequência de entrada de
tamanho arbitrário e gerar uma representação como saída, o seu estado interno,
representando em marrom no diagrama acima. O \textbf{Decoder} então recebe essa representação interna (também chamada
de \textit{codificação}) e usa-a para gerar saídas sequencialmente, podendo
então gerar essas sequências de duas formas: O Decoder pode usar as suas próprias saídas
como entrada para gerar a saída da próxima iteração temporal, ou então usar dados de treinamento como
entradas, essa última forma chamada de \textit{teacher forcing}. No diagrama
está indicado o acoplamento entre o decoder e o encoder. Essa codificação é apenas
um vetor (cuja dimensão é um \textit{hiper-parâmetro} de treinamento) que resume a
informação sequencial lida pelo encoder e a transmite para o decoder.
Hiper-parâmetros são parâmetros do problema de aprendizado que são definidos
pelo programador e não fazem parte dos valores aprendidos pelo algoritmo de
aprendizado. É comum que se realizem diversos experimentos com diferentes
valores de hiper-parâmetros e se escolha os que geraram melhores resultados. 
\\

Redes encoder-decoder são muito usadas para aprender \textit{representações} para os
dados. Isso quer dizer que as codificações da rede encoder-decoder são frutos de
transformações no espaço das entradas que buscam extrair a informação mais útil
para o objetivo do aprendizado. Podemos pensar nessas transformações como
mudanças de coordenadas nas quais estamos representando os dados. Justamente o problema de aprender
representações estimulou desde 2006 uma redescoberta do DL \citep{dlbook}. Criar
operações que transformem os dados de entrada de modo a facilitar o aprendizado
pode levar anos se essa tarefa for colocada na mão de especialistas humanos
\citep{dlbook}, então é uma capacidade valiosa proporcionada por algoritmos de
DL como redes encoder-decoder. \\



\section{Modelos Bayesianos}

Apesar de se tratarem de aproximações de verdadeiras inferências bayesianas, os
modelos nessa seção buscam unir a acurácia de métodos frequentistas de
Aprendizado Automático com a
margem de incerteza proveniente do uso da Lei de Bayes.

\subsection{Inferência Bayesiana em Aprendizado de Máquinha}
\label{sec:bayesinf}
O tratamento Bayesiano para modelos de Aprendizado Automático é bastante diverso dos frequentistas \citep{dlbook}.

Pode-se considerar que diferença entre as duas escolas de pensamento está em
\textbf{onde} se considera que a incerteza do problema está presente: Para a
estatística bayesiana a incerteza está presente no modelo, as observações não
são consideradas variáveis aleatórias, mas uma realidade observada. Para a estatística frequentista
se considera que os dados de entrada são uma realização possível de uma
distribuição desconhecida (uma variável aleatória), e nosso modelo gera \textbf{estimativas pontuais} \citep{rethink}.

Em uma análise frequentista estima-se um valor de $\theta$ e então todas as
predições são feitas a partir desse valor. No caso Bayesiano se consideram toda
a distribuição de possíveis valores de $\theta$ ao se fazer uma predição. \\ 


Sejam $X,Y$ conjutos de entradas e anotações para o treinamento de um modelo,
$x^*,y^*$ um par inédito de entrada e anotação (i.e. dados de teste) e
$\theta$ os parâmetros dos modelos. $p(y^* | x^*,\theta)$ é a verossimilhança de
uma nova predição do modelo e $p(\theta | X,Y)$ é a distribuição posterior dos
parâmetros do modelo.

Para obtermos a distribuição de uma nova
predição do modelo devemos marginalizar os parâmetros $\theta$ usando a seguinte integral:

\begin{equation}
  \label{eq:int}
  p(y^* | x^* , X,Y) = \int  p(y^* | x^*,\theta) p(\theta | X,Y)  d\theta 
\end{equation}

Para grande parte dos modelos de Aprendizem Automática a distribuição posterior
$p(\theta | X,Y)$ é intratável \citep{ubertime}. Pode-se de definir uma outra
distribuição $q(\theta)$, essa podendo ser otimizada por métodos numéricos para
aproximar $p(\theta | X,Y)$. E então usando a distribuição variacional podemos
calcular uma aproximação da distribuição das predições $q(y^*|x^*)$. 

\begin{align*}
  \label {eq:pq}
    p(y^* | x^* , X,Y) &\approx \int  p(y^* | x^*,\theta) q(\theta)d\theta \\
                       &= q(y^* | x^*)
\end{align*}

 Para aproximar $q^*$ da distribuição alvo usa-se o método da Inferência
Variacional \citep{bayesml}. O objetivo $\mathcal{L}_{VI}$ na
Equação~\ref{eq:ine} é sempre menor que $p(Y|X)$, e igual no limite em que
$q(\theta)$ seja exatamente igual a $q(\theta | X,Y)$. Esse objetivo é chamado de
\textit{Variational Lower Bound}, e ele é derivado da distância KL entre $q$ e
a distribuição alvo:

\begin{equation}
  \label {eq:ine}
  \mathcal{L}_{VI} = \int q(\theta) \log p(Y | X,\theta)d\theta - \mathnormal{KL}(q(\theta) ||p(\theta)) \\
                  \leq \log p(Y|X)
\end{equation}

A outra distância KL está presente no objetivo de otimização é a
distância entre a distribuição a priori $p(\theta)$ e a distribuição aproximada
$q(\theta)$ dos parâmetros do modelo. É importante notar que a Equação~\ref{eq:ine} é verdade para qualquer escolha de
$q$. A otimização de $\mathcal{L}_{VI}$ buscará tornar essa aproximação o melhor possível.


\subsubsection{Rede Neural Bayesiana}

Redes Neurais Bayesianas, como qualquer modelo bayesiano, especificam uma distribuição \textit{a priori} da matriz $\theta$ de parâmetros.
A Lei de Bayes é então aplicada para se calcular a distribuição posterior de
$\theta$, em vez de uma estimativa pontual, como usualmente é feito no tratamento frequentista.
Devido a complexidade desses modelos, é intratável calcular analiticamente essa distribuição posterior, então diversos métodos foram propostos para uma aproximação \citep{Gal2016Uncertainty}.
O Método aplicado nesse trabalho é o Monte Carlo Dropout, que não requer nenhuma mudança na arquitetura do modelo para realizar a inferência. Consistindo apenas de uma reinterpretação de uma Rede Neural implementada com uma técnica de regularização chamada Dropout. 

\subsubsection{Monte Carlo Dropout}

O processo de Inferência Variacional pode ser aproximado em uma rede neural pela técnica do Monte Carlo Dropout \citep{dropbayes}. \\

O Monte Carlo Dropout consiste no uso de \textbf{Dropout} em todas as camadas da rede
neural, i.e. descartar ativações aleatoriamente entre duas camadas da rede com
probabilidade $p$.\\

\begin{figure}
  \centering
  \resizebox {\columnwidth} {!} {\input{chapters/dropout.tex}}
  \label{fig:dropout}
  \caption{Representacão do uso de Dropout em uma rede neural}
\end{figure}


A cada computação da rede amostra-se com geradores de números aleatórios uma
máscara que ira zerar valores de ativação entre camadas subsequentes. Iremos
reparametrizar a rede neural com o fim de poder mostrar que o objetivo de
otimização da rede com dropout é equivalente ao método de inferência variacional.

Seja uma rede neural, cuja matriz de parâmetros é $\theta$, com dimensões
$K_i,K_{i-1}$, para cada camada $i$, e $L$ camadas. Define-se uma distribuição
de probabilidade sob a matriz de parâmetros $q(\theta)$.
Essa distribuição usa os pesos $M$ inicializados na rede e associa a eles a incerteza proveninente do
método de Dropout. A distribuição $q(\theta)$ se comporta da seguinte maneira,
onde $M_i$ e $p_i$, em cada camada, são considerados parâmetros variacionais:

\newcommand{\diag}{\mathop{\mathrm{diag}}}

\[
  \theta_i   = M_i   diag (z_{i,j}) \quad \text{para} \, j=1, \, \dots  \,K_i
\] 
\[
  z_{i,j}  \sim Bernoulli(p_i) \quad  \text{para} \, i=1, \, \dots \, L, \, j=1, \, \dots \, K_{i-1}
\]   


A função de custo de uma rede neural com dropout em todas as camadas  e
regularização L2 pode ser escrita da seguinte forma:

\[     \mathcal{L}_{dropout} = \sum^N_{N=1} [y_n - \mathnormal{f}^{\theta^*}(x_n)]^2 + \boldsymbol{\alpha}\{\theta^2\} \]


O objetivo de minimização desse Rede Neural com dropout em todas as camadas é
equivalente a diminuição da divergência KL entre $q(\theta)$ definido acima e a distribuição
posterior $p(\theta | X,Y)$ de um Processo Gaussiano Profundo que representa a
distribuição dos parâmetros da rede \citep{dropbayes}. O objetivo
$\mathnormal{L}$ da inferência variacional é escrito como:

\[
    \mathnormal{L} = - \sum^n_{n=1}\int{q(\theta)  \log p(y_n | \theta,x_n)}d\theta + \mathnormal{KL}(q(\theta),p(\theta))
\]


Cada termo do somatório é uma integral que será aproximada pelo método de Monte Carlo 
considerando-se apenas um termo, i.e. $\theta^* \sim q(\theta)$:

\begin{align*}
                    &\sum^N_{N=1} \int q(\theta) \log p(y_n | x_n,\theta)d\theta \\ 
                    &\qquad \qquad  \theta^* \sim q(\theta) \\
                    &\approx \sum^N_{N=1} \log p(y_n | x_n,\theta^*) \\
                    &= \sum^N_{N=1} [y_n - \mathnormal{f}^{\theta^*,b}(x_n)]^2
\end{align*}

A verossimilhança $\log p(y_n | \theta^*,x_n)$ é calculada pela
função de custo $L[y_n,\mathnormal{f}^{\theta^*}(x_n)]$, no exemplo apenas a distância quadrada
entre $y_n$ e a predição $\mathnormal{f}^{\theta^*}(x_n)$.

O segundo termo do objetivo de inferência é aproximado pela norma L2 dos
parâmetros da rede:

\begin{equation}
    \mathnormal{KL}(q(\theta) ||p(\theta)) \approx \boldsymbol{\alpha}\{\theta^2\}
 \end{equation} 

Então, após realizado o treinamento, a saída do nosso modelo é uma amostragem da
distribuição $q(y^* | x^*,X,Y)$. Se realizarmos $B$ computações de uma mesma
predição e calcularmos a média e variância dessa amostra, esses serão
estimadores não enviesados da média e variância de $p(y^*|x^*,X,Y)$, a
distribuição das predições do modelo:


\begin{align*}
  \label{eq:aproxs}
  \widetilde{\mathbb{E}}[y^*] &=
   \frac{1}{B}\sum^B_{B=1}\mathnormal{f}^{\hat{W},b}(x^*) = \frac{1}{B}\sum^B_{B=1}\hat{y}^*_{(B)}\\ 
   \widetilde{\mathit{Var}}[y^*]  &= \frac{1}{B}\sum^B_{B=1}(\hat{y}^*_{(B)} - \bar{y}^*)^2 
 \end{align*}


Esse método permite obter medida de incerteza a posteriori em modelos de rede
neural frequentistas sem nenhuma alteração no método de otimização.
Os resultados apresentados nessa sessão também valem para modelos de rede neural
mais complexos como LSTMs \citep{dropbayes}.
 
\subsubsection{Modelo Encoder-Decoder-Forecaster}

Iremos utilizar nesse trabalho o modelo proposto em \citep{ubertime}. A
arquitetura consiste em uma rede \textbf{encoder-decoder} que aprende codificações da
série temporal (i.e. uma representação que resuma informações úteis para o
problema) e então uma rede \textbf{forecaster} que use essas codificações ao lado de
variáveis exógenas a série temporal para realizar predições.  



\begin{figure}[H]
\centering
\includegraphics[width=0.9\columnwidth]{uber-train.png}
\caption{Modelo durante o Treinamento \citep{ubertime}}
\label{img:uber1}
\end{figure}


Durante o pré-treinamento a rede encoder-decoder consome sequências de $F + T$ dias
da série temporal. O encoder cria uma representação vetorial $h$ depois de
receber como entrada os primeiros $T$ dias da sequência. Então, $h$ é usado como
inicialização do estado interno do decoder, e esse então consume mais $F$
entradas da sequência. Para o decoder, se em uma iteração sua entrada é $X_i$,
então sua saída será comparada com $X_{i+1}$, e esse erro é propagado após lidas
todas as entradas para que a representação $h$ proveniente do encoder possa se
tornar mais informativa.


\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\columnwidth]{uber-predict.png}
  \caption{No módulo de inferência usamos uma rede neural simples para gerar
    predições a partir da codificação aprendida durante o pré-treino \citep{ubertime}}
  \label{img:uber2}
\end{figure}




Esse modelo também possui outra característica importante. Todas as camadas da
redes neurais que compõe o encoder, o decoder e o forecaster possuem
Dropout com probabilidade $p$. Ou seja, podemos usar a técnica do Monte
Carlo Dropout para estimar a variância de cada predição feita por esse modelo. A
rede então se torna um modelo bastante robusto para consumir séries temporais,
assim realizando predições e incertezas.\\


Essa capacidade do modelo o torna mais interessante do que meramente usar uma
rede LSTM para o problema de regressão. Agora iremos definir como a informação
de incerteza pode ser calculada usando esse modelo. \\

Para esse problema usamos a verossimilhança $p(y |f^\theta(x))$.
Onde $f^\theta(.)$ é a rede Encoder-Forecaster após o treino.
Como estamos lidando com regressão, podemos especificar ainda essa distribuição
como \citep{ubertime}:  

\begin{equation}
  \label{eq:reg}
 p(y| \theta,x) \sim \mathcal{N}(f^{\theta}(x),\,\sigma^2)
\end{equation}

Desejamos encontrar a distribuição $p(y^*| x^*)$ para dados inéditos. Seguindo
a formalização bayesiana, é necessário marginalizar os parâmetros do modelo da
verossimilhança, i.e. resolver a Integral~\ref{eq:int}. Para esse modelo, como
para muitos outros de interesse, essa integral é intratável. No trabalho
\citep{ubertime}, os autores estudam a variância de $p(y^{*} | x^{*})$ que
quantifica a incerteza das predições. Primeiro essa variância é decomposta pela
lei da variância total, e seus fatores são aproximados independentemente: 

\begin{equation}
   Var[y^* | x^*] = Var[\mathop{\mathbb{E}}(y^* | \theta,x^*)] + \mathop{\mathbb{E}}[Var(y^* | \theta,x^*)]
   \label{eq:vartot}
 \end{equation}

$\mathbb{E}[y^* | \theta,x^*]$ é a verossimilhança do modelo, ou seja:

\[
  Var[\mathbb{E}(y^* | \theta,x^*)] = Var[f^\theta(x^*)]
\]


$Var[f^\theta(x^*)]$ representa a incerteza do modelo, e a estimativa desse valor será feito pela técnica do MC Dropout. A variância
será aproximada pela variância amostral de $B$ predições estocásticas calculadas
na rede com camadas de Dropout ativadas. Seja $\{\hat{y}^*_{(1)},\hat{y}^*_{(2)}, \dots
\hat{y}^*_{(B)}\}$ o vetor de predições amostrado dessa forma, e $\hat{y}^*$ sua
média amostral, temos: \\

\[   Var[f^\theta(x^*)] \sim \eta_1^2 = \frac{1}{B}\sum^B_{B=1}(\hat{y}^*_{(B)} - \hat{y}^*)^2  \]

O termo $\mathop{\mathbb{E}}[Var(y^* | \theta,x^*)]$ da Equação~\ref{eq:vartot}, o
ruído inerente do modelo, será estimado com um conjunto de dados de validação.
Sejam $(X',Y')$ dados de validação, com
$V$ entradas. Esses dados são independentes do modelo
treinado $f^\theta(.)$, então podemos usá-los para estimar o ruído inerente do
modelo: \\

\[
  \mathop{\mathbb{E}}[Var(y^* | \theta,x^*)] \sim \eta_2^2 = \frac{1}{V}\sum^V_{V=1}(y'_v - f^\theta(x'_v))^2
\]



A Equação~\ref{eq:vartot} fica resumida em:


\[ Var[y^* | x^*] \sim \eta^2_1 + \eta^2_2 \] 

O desvio-padrão total do modelo é então calculado por:

\[
  \eta = \sqrt{\eta^2_1 + \eta^2_2}  
\]


\subsection{Modelo DeepAR}

O modelo \textit{DeepAR}, proposto em \citep{deepar}, é baseado em redes neurais recorrentes auto-regressivas,
i.e. que tem sua saída realimentada na entrada da iteração posterior para
poder gerar predições em um horizonte de tempo arbitrário. Por meio da função de verossimilhança binomial esse modelo é capaz
de realizar predições probabilísticas.

O modelo é descrito pela seguinte equação, onde $h$ é uma RNN implementada com células de LSTMs:

\[
h_{i,t} = h(h_{i,t-1},y^*_{i,t-1},x_{i,t}, \theta)
\]

O modelo é treinado em um conjunto de diversas séries temporais $y_i$ do
processo em estudo, todas representando o mesmo período de tempo. Nota-se que o estado da RNN tem como argumento o estado anterior e os parâmetros de entrada, mas também a saída da iteração passada do modelo, $y^*_{i,t-1}$. \\

A verossimilhança do modelo, $p(y_{i,t} | \mu,\sigma)$, é uma distribuição fixa
cujos parâmetros $\mu, \sigma$ são dados por uma função determinística da saída $h$ do modelo. Usaremos uma função de verossimilhança binomial já que estamos otimizando um objetivo de regressão, assim como na Equação~\ref{eq:logver1}:
\[
  p(y | \mu,\sigma) = {(2\pi\sigma^2)}^{-\frac{1}{2}} \exp(-  \frac{-y - \mu^2}{2\sigma^2})  
\]

A média $\mu$ e o desvio-padrão $\sigma$ são calculados diretamente pela saída do modelo. Para a média, usamos uma transformação linear parametrizada por $W_{\mu},b_{\mu}$. No caso do desvio-padrão, para garantirmos que ele seja maior que 0, usamos uma função \textit{softplus} após uma transformação linear análoga ao caso da média, parametrizada por  $W_{\sigma},b_{\sigma}$.\\

\begin{alignat}{3}
  \mu(h_{i,t}) &= W_{\mu}h_{i,t} + b_{\mu} \\ 
  \sigma(h_{i,t}) &= \log(1 + \exp(W_{\sigma}h_{i,t}+ b_{\sigma}))
\end{alignat}


\subsubsection{Treinamento}


O modelo possui uma etapa de treinamento e uma etapa de validação. O Diagrama~\ref{fig:deepartrain} ilustra como o modelo se comporta em cada um desses momentos: \\


\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\columnwidth]{deepar-train.pdf}
  \caption{Para o treino, o modelo é guiado por anotações reais de valores
    passados da série alvo $y$ e as variáveis independentes $x$. Usamos o estado
  $h$ da rede LSTM a cada iteração temporal para calcular $\mu$  e $\sigma$.
  Esses valores são então usados no calculo da verossimilhança $p(y |
  \mu,\sigma)$, o valor que é minimizado.}
  \label{fig:deepartrain}
\end{figure}

%% lembrar que p(y | \mu,\sigma) == l(z| \theta) no diagrama!
%%

O treinamento do modelo é feito pela maximização da log-verossimilhança: \\

\begin{equation}
  \mathcal{L} = \sum^n_{t=1}{\log p(y_{i,t} | \mu_{i,t},\sigma_{i,t})}
  \end{equation}

Durante a etapa de predição, os valores calculados de $\mu$ e $\sigma$ são
usados como parâmetros de uma distribuição normal que caracteriza a distribuição
de cada predição $y^*$.


\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\columnwidth]{deepar-pred.pdf}
  \caption{Se desejarmos prever mais de um valor da série-alvo $y$ no futuro. O
    modelo é realimentado com os valores amostrados de $y^*$ e a rede continua a
  realizar predições.}
  \label{fig:deepartrain}
\end{figure}


Uma saída $y^*_{i,t}$ do modelo é calculada por meio de uma amostragem da Gaussiana determinada pela média e variância calculadas: \\

\[
 y^*_{i,t} \sim \mathcal{N}(\mu_{i,t}\sigma_{i,t}) 
\]

\subsection{Modelo com Deep Factors}

O modelo \textit{Deep Factors with Gaussian Process} \citep{deepfactors} separa o problema de predição em uma parte \textbf{local}, modelada por um Processo Gaussiano que gera incertezas, e uma parte \textbf{fixa}, modelada por uma rede neural recorrente treinada em diversas séries temporais do mesmo domínio.


Um Processo Gaussiano é uma coleção de variáveis aleatórias,
sendo que qualquer subconjunto finito das mesmas é distribuído normalmente \citep{gpml}. \\

Um Processo Gaussiano é descrito completamente pela sua função de média e sua função de covariância. Usamos essas duas funções para amostrar o processo estocástico $f(x)$ que está sendo modelado: \\

\[
f(x) \sim \mathcal{GP}( m(x), K(x,x'))
\]

É comum que se escolha como função média apenas a constante 0 \citep{gpml}.  A função de covariância, as vezes chamada de Kernel, deve especificar a covariância entre os pares de variáveis aleatórias (i.e. os dados), para problemas de regressão é usual escolhermos um Kernel que relacione uma medida de distância das variáveis de entrada. Como por exemplo o \textit{quadrado da função exponencial}: \\

\[
  K(x,x') = \exp(-\frac{1}{2}\abs{x - x'}^2)
\]


\subsubsection{Modelo Generativo}

O modelo Deep Factors então é composto por duas partes, a combinação linear de ambas é a saída do modelo i.e. a emissão de valores de saída amostrados: \\


A função $g_t$ é uma RNN, no caso desse trabalho uma célula LSTM, que recebe como entrada um certo número $T$ de passos da série temporal, e então emite como saída um horizonte finito $F$ de predições.

\[
  \textbf{Gerador da parte fixa aprendida globalmente: }  f_{i,t} = W_ig_t(x_{i,t})
\]

O ruído é gerado por um Processo Gaussiano com média 0 e função de covariância
exponencial. O Processo gera incertezas a partir da matriz de covariância
calculada com todas as predições da parte global do modelo. Seja $f_i$ um vetor
de predições com $T$ entradas calculado pela rede LSTM para a série temporal $i$, a matriz de covariância
terá a forma:  

\[
  \begin{bmatrix}
    1& K(f_{i,1},f_{i,2}) & \dots &K(f_{i,1},f_{i,T}) \\ 
    K(f_{i,2},f_{i,1}) &1  & \dots & K(f_{i,2},f_{i,T})\\ 
    \vdots &  \vdots  & \ddots & \vdots\\
    K(f_{i,T},f_{i,1})& K(f_{i,T},f_{i,2}) & \dots & 1 
  \end{bmatrix} 
\]
\[
    \textbf{Gerador de ruído local: }  r_i \sim \mathcal{GP} (0, K_i(.,.))
\]

A emissão ira combinar efeitos globais e ruído local para gerar as predições
finais amostrando-se a variável latente $u$:
  
\[
  \textbf{Emissão: }  y^*_{i,t} \sim p(. | u_{i,t}) , u_{i,t} = r_{i,t} + f_{i,t}  
\]

\subsubsection{Treinamento}

Como $p(. | u_{i,t})$ é distribuida normalmente, não precisamos de inferência para calcular a verossimilhança marginal do modelo, que é dada por: \\

\[
p(y_{i}) = \mathcal{N}(f_i,K_i + \sigma_i^2\mathcal{I})
\]

O objetivo de treinamento é achar o conjunto de parâmetros da RNN e do Processo Gaussiano que maximizem a log-verossimilhança dos dados: \\
\[
\mathcal{L} = \sum_{t=0}^N{\log p(y_t|x_t)}
\]


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../quali"
%%% TeX-command-default: "latexmk"
%%% bibtex-file-path: "../bibliografia"
%%% End: