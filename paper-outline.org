

#+TITLE: Using Deep Learning Bayesian Methods do Predict Cement Compressive Strength with Uncertainty  
#+SUBTITLE: 
#+AUTHOR: Thiago Lira 
#+DATE: May 1, 2019
#+OPTIONS: toc:nil 

#+BIBLIOGRAPHY: bibliografia plain

#+LaTeX_HEADER: \usepackage{amsmath,amssymb}
#+LaTeX_HEADER: \usepackage{empheq}

#+begin_abstract
Much of the past work on the prediction of cement compressive strength (CSC) fail to account for the reality of the industrial process (e.g. which data is avaliable on a daily basis),
instead providing a limited analysis on isolated samples on a laboratory. Recent work considered the cement data as indexed by time, providing 
models that could be used on a daily basis on the factory floor, providing decision making capabilities for the stability of the industrial process, using uncertainty.
This work goes further and implements Bayesian Deep Learning techniques on this domain. These are able to scale to huge amounts of data, require no
specific knowledge of the data and provides powerful uncertainty estimates, able to be calculated and renewed every day with the new data being produced on the factory.
#+end_abstract


** Introduction
The prediction of the compressive strength of cement (CSC) is of the utmost importance on the Civil Construction Industry, 
given that this capacity is one of the factors to assess the quality of cement cite:cementnn1.

The 28-day compressive strength (RC28) is choosen as a proxy to measure the quality of the cement, and the variance of these measurements can indicate the stability of the industrial proccess. 

It is important for the industry to have a system that, detecting drops on the quality of the cement, can point to anomalies on the proccess (e.g. concentration of reagents) 
probably causing it. The problem can them be tackled with supervised learning cite:dlbook, where the target are the RC28 measurements and the inputs are chemical compositions and other 
quantities measured on a daily basis on the factory.

Much of the past work cite:cementlin,nncement using mathematical models to predict CSC have failed to account for the dynamical nature of the industrial process and instead rely on analises that are hard to make 
on a daily basis on the industrial floor cite:dynstat. 

Past work on this more industry-aware front cite:greciaLin makes the distinction between dynamic and static models to predict the CSC. 
The authors propose a dynamic moving window linear regression that recalculates it's regression parameters every time a new RC28 measurement is made. 
This dynamical model is also able to provide standard deviations for the regression parameters.

Following that idea, we shall consider all the cement data as time series (i.e. indexed by time), and model them as such, 
with the assumption that the past of the measurements have some information concerning the future of the process. 

Going further, many time series problems on different domains had it's state of the art results beaten by new Deep Learning Recurrent Neural Network models cite:energylstm,lstmbr,
these deep learning models require no /feature engineering/ and work very well even with gigabytes of data.

Classic time series models fail to scale well to huge amounts of data, although they are able to provide good uncertainty estimates cite:deepar. A new trend is to apply Bayesian Deep Learning
techniques to leverage the scalability of deep learning with the uncertainty estimation of Bayesian Statistics. This last feature is particularly useful on a industrial setting, as to provide 
decision making evidence on the factory floor cite:deepfactors. We shall bring these new Bayesian Deep Learning capabilities to the CSC prediction domain. 

The training data is composed of the pairs $(\{\textbf{x}_{t_0},y_{t_o}\},\{\textbf{x}_{t_1},y_{t_1}\}, \dots, \{\textbf{x}_{T},y_{T}\})$ on the time span $[t_o,T]$. 
Let F be a finite time horizon, such that F > T, the models should learn a probability distribution of the form:

#+BEGIN_EXPORT latex
\begin{equation}
p(y_{T:F} | y_{t_{o}:T},\textbf{x}_{t_{0}:T}) 
\end{equation} 
#+END_EXPORT 

We shall make a quick review of such recent literature:

- Deep Factors cite:deepfactors : The Deep Factors Model combines a LSTM time series regression model to model the "global" part of the predictions, and a Gaussian Process to model the "local" part
  i.e. the noise and uncertainty.
 
- DeepAR cite:deepar : This model uses an autoregressive RNN architecture to model the probability distribution of a time series. The likelihood function can be choosen given the statistical 
  characteristics of the problem. For this problem we use a Gaussian Likelihood.

- Encoder Decoder Forecaster cite:ubertime  : This model proposed by Uber implements a Bayesian Neural Network (BNN) via the Monte Carlo Dropout technique (a variational inference aproximation), 
  which them enables the model to decompose it's prediction uncertainty into three types:
  model uncertainty, inherent noise and model misspecification.
  

 
** Data 

The data represents 11 years of cement production on a medium sized factory, located on the city of Cajati.  

For the models, we use the data from the last phase of the production of cement. The measurements represent the cement when it is ready to be shipped. 
On this particular dataset, we have 2408 almost daily anotations spanning 8 years of cement production, with 25 collums (i.e. features) per day.

From this data, we remove the columns of measurements that had a frequency too little to be of use statistically (i.e. < 70% of all days with data).

The data is first resampled (and filled with the mean in the case of missing values) so that we don't have any days without measurements. 
Since we are dealing with inputs of different orders of magnitude, we first rescale the data to be in the [0,1] range with the minmax method. Let  $\textbf{x}$ be a column of data, it's 
normalized value $\textbf{z}$ will be:


#+BEGIN_EXPORT latex
\begin{equation}
z=\frac{x-\min (x)}{\max (x)-\min (x)}
\end{equation}
#+END_EXPORT

After cleaning, the columns with enough data to be useful at the learning task are: 

 - (i) Percentages related to chemical composition: AL_{2}O_3, SIO_2, MGO, RICARB, P_{2}O_5 and FE_{2}O_3 
 - (ii) Percentage of the matter of water relative to the cement: AGP
 - (iii) Time (in seconds) that the material takes to begin hardening and to finish hardening, respectively: IP, FP
 - (iv) Blaine fineness, measured in square centimeters per gram: SBL
 - (v) Percentage of Mass lost on furnace: FP
 - (vi) The CSC measurements made after 3, 7 and 28 (our target) days of expedition of that particular sample: RC3, RC7, RC28


Using the Machine Learning approach cite:dlbook,  we shall let the model learn by itself what parameters to give more importance, 
thus not doing any /feature engineering/ prior to training the model, i.e. we shall not 
use any civil engineering or chemistry knowledge to alter the models or the data in any way.

So, every day $t$ a new lot of cement is ready and expedited from the factory. This lot doesn't have RC3, RC7 or RC28 measurements yet made. The task is to predict with a margin of confidence 
the RC28 value of this specific lot. From this lot we have multiple concentrations and quantities of reagents annotated from the factory as inputs.

Following the lead of cite:greciaLin, we will have the RC3 and RC7 measurements along with the other inputs. But if we are predicting the compressive strength of day $t$, the RC3 and RC7 measurements
will come from the last lots of cement of which this measurement is avaliable on that day i.e. from the lots of the days $t-3$ and $t-7$ respectively. This is to ensure that such analysis
can be done with new day, on the day of it's expedition, thus not confining our analysis just to older samples i.e. ones from which we already know it's RC3, RC7 and RC28 measurements. 

We shall use data from 01/2007 to 09/2018 as our training data, and the last 3 months of 2018 (where our data ends) as our validation data, as we are assuming that the past of the process
can give information about it's future.

We shall then evaluate how long in the future can the models reliably predict new measures with an acceptable uncertainty.
It's expected that the validation error will increase the farther we try to predict the RC28 measure on the future.

** Experiments

All models were implemented using PyTorch cite:pytorch, for the Gaussian Processes we used GPyTorch cite:gpytorch. The hyperparameters chosen for each model are shown on Table [?]. 
 
Every RNN based model will have a certain window of data as input to predict the next day of RC28. The size of this window is fixed across models so that we might compare it's accuracies. 
 
The models forecast performance will be evaluated by two metrics: One of which, the Quantile Loss (or \rho-risk), will assess the quality of the uncertainty measure,
and the RMSE will assess the error. 
 
Given a true value y_{t} and a quantile prediction y^*_t(\rho), with  1 > \rho > 0. The Quantile Loss is defined as:


#+BEGIN_EXPORT latex
\begin{equation*}
  \mathcal{QL}_{\rho}(y_{t},y^{*}_{t}(\rho)) =
\begin{cases}
  2 \rho(y_{t} - y^{*}_{t}(\rho)) & \text{if }  y_{t} - y^{*}_{t}(\rho) > 0 \\
  2 (1 - \rho)(y^{*}_{t}(\rho) - y_{t}) & \text{if } y_{t} - y^{*}_{t}(\rho) \leq 0
\end{cases}
\end{equation*}
#+END_EXPORT

To compare the models we shall use the normalized sum of quantile losses, or \rho-risk. 
\\
#+BEGIN_EXPORT latex
\begin{equation*}
\sum_{t}\frac{\mathcal{QL}_{\rho}(y_{t},y^{*}_{t})}{\sum_{t}y_{t}}
\end{equation*}

#+END_EXPORT

We will use the values of \rho 0.5 and 0.9. These values can be understood as the expected error on 50% and 90% of the sampled values, respectively. 
The .5-risk is equivalent to the mean absolute percentage error (MAPE).


The RMSE loss is defined as:

#+BEGIN_EXPORT latex
\begin{equation*}
\sum^n_{t}\sqrt{\frac{(y_t - y^*_{t})^2}{n}}
\end{equation*}
#+END_EXPORT

We shall compare the accuracy of the models as time passes. The models are trained with data containing some last day T, 
the validation data follows imediatly after T, and we evaluate the model's average accuracy with a increasing number of days of prediction after T.

#+BEGIN_center
# #+CAPTION: RMSE as a function of the date using the model Deep AR
#+ATTR_LaTeX: :height 0.3\textwidth :center
[[file:~/Dropbox/Mestrado/Intercement/paper_img/rmse_deep_ar.pdf]] 
#+ATTR_LaTeX: :height 0.3\textwidth :center
[[file:~/Dropbox/Mestrado/Intercement/paper_img/rmse_deep_factors.pdf]] 
#+ATTR_LaTeX: :height 0.3\textwidth :center
[[file:~/Dropbox/Mestrado/Intercement/paper_img/rmse_enc_dec.pdf]] 
#+END_center

We observe then that error steadily increases as time passes, as in cite:dynstat, but on a much slower rate. The RNN models used on this work have the capability to 
dinamically weight the last $t_f$ days ($t_f$ is a hyper-parameter) to make the best prediction, so we don't need to update the models parameters as often as a moving window linear regression.

We now plot the predictions for 90 days after T of the models against it's true values, to evaluate the distribution of the predicted values.

#+BEGIN_center
# #+CAPTION: Scatter Plot of the Predictions as a function of the True Values
#+ATTR_LaTeX: :height 0.3\textwidth :center
[[file:~/Dropbox/Mestrado/Intercement/paper_img/qq_deep_ar.pdf]] 
#+ATTR_LaTeX: :height 0.3\textwidth :center
[[file:~/Dropbox/Mestrado/Intercement/paper_img/qq_deep_factors.pdf]] 
#+ATTR_LaTeX: :height 0.3\textwidth :center
[[file:~/Dropbox/Mestrado/Intercement/paper_img/qq_enc_dec.pdf]] 
#+END_center

The Encoder-Decoder models seems to be able to best model the target distribution.

To evaluate the quality of the uncertainty measures, we shall use the .5 risk and .9 risk metrics. For each model 
we wil compare the risks for the predictions of the next day, the next 3 days and the next 7 days. 

#+BEGIN_center
#+NAME: table-yield
#+CAPTION: Simple table created using LaTeX tabular environment
#+attr_latex: :environment tabular :width \textwidth :align lrr
| Encoder Decoder | .5 risk | .9 risk |
|             24h |   0.004 |   0.025 |
|              3d |   0.005 |    0.02 |
|              7d |   0.011 |   0.037 |

|    Deep Factors | .5 risk | .9 risk |
|             24h |   0.001 |   0.036 |
|              3d |   0.009 |   0.031 |
|              7d |   0.023 |   0.027 |

|         Deep AR | .5 risk | .9 risk |
|             24h |   0.009 |   0.004 |
|              3d |   0.018 |   0.008 |
|              7d |   0.044 |   0.016 |

#+END_center

** Conclusion

This work has applied Bayesian Deep Learning techniques to the modeling of uncertainty of cement strength prediction. The results of the RMSE measures are similar to state of the art results using
linear regression based techniques. We present results of uncertainty metrics using multiple models and time horizons, to serve as a benchmark for future work on this direction. 
We have not been provided with enough data to be comparable with most public datasets used to test Deep Learning models, with that much data, the Deep Learning approach is more resource intensive 
and prone to variance problems than the models used on previous work on this task. On the future however, with more data, the Deep Learning approach should be more prevalent on this industry.

#+BEGIN_EXPORT latex
\bibliographystyle{plain}
\bibliography{bibliografia}{}
#+END_EXPORT 
