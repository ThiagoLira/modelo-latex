

#+TITLE: Using Deep Learning Bayesian Methods do Predict Cement Compressive Strength with Uncertainty  
#+SUBTITLE: 
#+AUTHOR: Thiago Lira 
#+DATE: May 1, 2019
#+OPTIONS: toc:nil 

#+BIBLIOGRAPHY: bibliografia plain

#+LaTeX_HEADER: \usepackage{amsmath,amssymb}
#+LaTeX_HEADER: \usepackage{empheq}
#+LaTeX_HEADER: \usepackage{float}
#+begin_abstract
Much of the past work on the prediction of cement compressive strength (CSC) fail to account for the reality of the industrial process (e.g. which data is avaliable on a daily basis),
instead providing a limited analysis on isolated samples on a laboratory. Recent work considered the cement data as indexed by time, providing 
models that could be used on a daily basis on the factory floor, providing decision making capabilities for the stability of the industrial process, using uncertainty.
This work goes further and implements Bayesian Deep Learning techniques on this domain. These are able to scale to huge amounts of data, require no
specific knowledge of the data and provides powerful uncertainty estimates, able to be calculated and renewed every day with the new data being produced on the factory.
#+end_abstract


** Introduction
The prediction of the compressive strength of cement (CSC) is of the utmost importance on the Civil Construction Industry, 
given that this capacity is one of the factors to assess the quality of cement cite:cementnn0.

The 28-day compressive strength (RC28) is chosen as a proxy to measure the quality of the cement, and the variance of these measurements can indicate the stability of the industrial process. 

It is important for the industry to have a system that, detecting drops on the quality of the cement, can point to anomalies on the proccess (e.g. concentration of reagents) 
probably causing it. The problem can them be tackled with supervised learning cite:dlbook, where the target are the RC28 measurements and the inputs are chemical compositions and other 
quantities measured on a daily basis on the factory.

Much of the past work cite:cementlin,nncement using mathematical models to predict CSC have failed to account for the dynamical nature of the industrial process and instead rely on analises that are hard to make 
on a daily basis on the industrial floor cite:dynstat. 

Past work on this more industry-aware front cite:greciaLin makes the distinction between dynamic and static models to predict the CCS. 
The authors propose a dynamic moving window linear regression that recalculates it's regression parameters every time a new RC28 measurement is made. 
This dynamical model is also able to provide standard deviations for the regression parameters.

Following that idea, we shall consider all the cement data as time series (i.e. indexed by time), 
with the assumption that the past of the measurements have some information concerning the future of the process.
 
Our objective is then to model the *full probability distribution* of the time series, as to provide decision making capabilities on the industrial floor.

For many decades the state-of-the-art on forecasting has been the ARIMA class of models, and Box-Jenkins procedures cite:inbook, but these models have their limitations. 
They are unable to model non-linear relationships on the data cite:forecasting, and many real world processes happen to be non-linear. These models also have trouble using exogenous variables 
to aid the time series forecasting cite:ubertime.


Recently many time series problems on different domains had it's classical results beaten by new Deep Learning methods such as Recurrent Neural Network models cite:energylstm,lstmbr,
these deep learning models require no /feature engineering/ and work very well even with gigabytes of data cite:ARIMA_LSTM. They are more data-driven than classical methods
that assume very little about the data, meaning that little manual 
tweaking on the model parameters is necessary.

Classic time series models fail to scale well to huge amounts of data, although they are able to provide good uncertainty estimates cite:deepar. Past work cite:DIAZROBLES20088331,KHASHEI2010479 
has managed to combine ARIMA models with Artificial Neural Networks (ANN), with better results than just either of these models.

We then use a new aproach, using Bayesian Deep Learning techniques to leverage the scalability of deep learning with the uncertainty estimation of Bayesian Statistics. 
This last feature is particularly useful on a industrial setting, as to provide decision making evidence on the factory floor cite:deepfactors. 

** Objectives

We shall bring these new Bayesian Deep Learning techniques to the CSC prediction domain. The contribuitions of this paper are 
as follows: 

- A comparison of 3 state-of-the-art Bayesian Deep Learning models for time series forecasting.
- A novel application of Bayesian Deep Learning on CCS, with performance similar to previous state-of-the-art results using linear regression.

** Preliminaries
  
The training data is composed of the pairs $(\{\textbf{x}_{t_0},y_{t_o}\},\{\textbf{x}_{t_1},y_{t_1}\}, \dots, \{\textbf{x}_{T},y_{T}\})$ on the time span $[t_o,T]$. 
Let F be a finite time horizon, such that F > T, the models should learn a probability distribution of the form:

#+BEGIN_EXPORT latex
\begin{equation}
p(y_{T:F} | y_{t_{o}:T},\textbf{x}_{t_{0}:T}) 
\end{equation} 
#+END_EXPORT 

We shall briefly explain each model used, where always the model with trained parameters \theta is represented by the function f^{\theta}. 

*** Deep Factors cite:deepfactors  

The Deep Factors Model combines a LSTM time series regression model to model the "global" part of the predictions, and a Gaussian Process to model the "local" part
i.e. the noise and uncertainty.
 
*** DeepAR cite:deepar  
  
This model uses an autoregressive RNN architecture to model the probability distribution of a time series. The likelihood function can be choosen given the statistical
characteristics of the problem. For this problem we use a Gaussian Likelihood.

The model is described via the following autoregressive equation:

#+BEGIN_center
h_{i,t} = h(h_{i,t-1},y^*_{i,t-1},x_{i,t}, \theta)
#+END_center
The hidden state **h** of a timestep is then used to calculate the mean 
and standard deviation of it's prediction:

#+BEGIN_center
\mu(h_{i,t}) = W_{\mu}h_{i,t} + b_{\mu} 

\sigma(h_{i,t}) = \log(1 + \exp(W_{\sigma}h_{i,t}+ b_{\sigma})) 
#+END_center
It is then a simple matter of using a Gaussian Likelihood with parameters \mu and \sigma to 
calculate the model's cost function and propagate it's error.

*** Encoder Decoder Forecaster cite:ubertime   
This model proposed by Uber implements a Bayesian Neural Network (BNN) via the Monte Carlo Dropout technique (a variational inference aproximation),
which them enables the model to decompose it's prediction uncertainty into three types:
model uncertainty, inherent noise and model misspecification. We assume a Gaussian Likelihood as
our data generating distribution: 

#+BEGIN_EXPORT latex
\begin{equation}
y| \theta \sim \mathcal{N}(y;f^{\theta}(x),\,\sigma^2)
\end{equation}
#+END_EXPORT 

Following the Bayesian aproach, we would integrate over the distribution of
our model parameters \theta, to get to the distribution of the predictions given new inputs 
p(y^* | x^*). The variance of this distribution can be calculated via the Law of Total Variance:

#+BEGIN_EXPORT latex
\begin{equation}
Var[y^* | x^*] = Var[f^{\theta}(x^*)] + \sigma^2  
\end{equation}
#+END_export

Where \sigma (the inherent noise) is estimated using the validation data and Var[f^W(x^*)] (the model uncertainty)
is given by the Monte Carlo Dropout Technique. This value is estimated via the sample variance 
of **B** stochastic realizations of a prediction, using the trained model: 

#+BEGIN_EXPORT latex
\begin{equation}
Var[f^{\theta}(x^*)]  = \frac{1}{B}\sum^B_{B=1}(\hat{y}^*_{(B)} - \hat{y}^*)^2  
\end{equation}
#+END_EXPORT 

** Methodology 

*** Data 
    
The data represents 11 years of cement production on a medium sized factory, located on the city of Cajati.  

For the models, we use the data from the last phase of the production of cement. The measurements represent the cement when it is ready to be shipped. 
On this particular dataset, we have 2408 almost daily anotations spanning 11 years of cement production, with 25 collums (i.e. features) per day.

From this data, we remove the columns of measurements that had a frequency too little to be of use statistically (i.e. < 70% of all days with data).

The data is first resampled (and filled with the mean in the case of missing values) so that we don't have any days without measurements. 
Since we are dealing with inputs of different orders of magnitude, we first rescale the data to be in the [0,1] range with the minmax method. Let  $\textbf{x}$ be a column of data, it's 
normalized value $\textbf{z}$ will be:


#+BEGIN_EXPORT latex
\begin{equation}
z=\frac{x-\min (x)}{\max (x)-\min (x)}
\end{equation}
#+END_EXPORT

After cleaning, the columns with enough data to be useful at the learning task are: 

 - (i) Percentages related to chemical composition: AL_{2}O_3, SIO_2, MGO, RICARB, P_{2}O_5 and FE_{2}O_3 
 - (ii) Percentage of the matter of water relative to the cement: AGP
 - (iii) Time (in seconds) that the material takes to begin hardening and to finish hardening, respectively: IP, FP
 - (iv) Blaine fineness, measured in square centimeters per gram: SBL
 - (v) Percentage of Mass lost on furnace: FP
 - (vi) The CSC measurements made after 3, 7 and 28 (our target) days of expedition of that particular sample: RC3, RC7, RC28


Using the Machine Learning approach cite:dlbook,  we shall let the model learn by itself what parameters to give more importance, 
thus not doing any /feature engineering/ prior to training the model, i.e. we shall not 
use any civil engineering or chemistry knowledge to alter the models or the data in any way.

**** RC3, RC7 and RC28
Every day $t$ a new lot of cement is ready and expedited from the factory. This lot doesn't have RC3, RC7 or RC28 measurements yet made. The task is to predict with a margin of confidence 
the RC28 value of this specific lot. From this lot we have multiple concentrations and quantities of reagents annotated from the factory as inputs.

Following the lead of cite:greciaLin, we will have the RC3 and RC7 measurements along with the other inputs. But if we are predicting the compressive strength of day $t$, the RC3 and RC7 measurements
will come from the last lots of cement of which this measurement is avaliable on that day i.e. from the lots of the days $t-3$ and $t-7$ respectively. This is to ensure that such analysis
can be done with new day, on the day of it's expedition, thus not confining our analysis just to older samples i.e. ones from which we already know it's RC3, RC7 and RC28 measurements. 

**** Train/Test Separation
We shall use data from 01/2007 to 09/2018 as our training data, and the last 3 months of 2018 (where our data ends) as our validation data, as we are assuming that the past of the process
can give information about it's future.

We shall then evaluate how long in the future can the models reliably predict new measures with an acceptable uncertainty.
It's expected that the validation error will increase the farther we try to predict the RC28 measure on the future.

** Experiments

All models were implemented using PyTorch cite:pytorch, for the Gaussian Processes we use GPyTorch cite:gpytorch. The hyperparameters chosen for each model are shown on Table [?]. 
 
Every RNN based model will have a certain window of data as input to predict the next day of RC28. The size of this window is fixed across models so that we might compare it's accuracies. 
 
The models forecast performance will be evaluated by two metrics: One of which, the Quantile Loss (or \rho-risk), will assess the quality of the uncertainty measure,
and the RMSE will assess the error. 
 
Given a true value y_{t} and a quantile prediction y^*_t(\rho), with  1 > \rho > 0. The Quantile Loss is defined as:


#+BEGIN_EXPORT latex
\begin{equation*}
  \mathcal{QL}_{\rho}(y_{t},y^{*}_{t}(\rho)) =
\begin{cases}
  2 \rho(y_{t} - y^{*}_{t}(\rho)) & \text{if }  y_{t} - y^{*}_{t}(\rho) > 0 \\
  2 (1 - \rho)(y^{*}_{t}(\rho) - y_{t}) & \text{if } y_{t} - y^{*}_{t}(\rho) \leq 0
\end{cases}
\end{equation*}
#+END_EXPORT

To compare the models we shall use the normalized sum of quantile losses, or \rho-risk. 

#+BEGIN_EXPORT latex
\begin{equation*}
\sum_{t}\frac{\mathcal{QL}_{\rho}(y_{t},y^{*}_{t})}{\sum_{t}y_{t}}
\end{equation*}

#+END_EXPORT

We will use the values of \rho 0.5 and 0.9. These values can be understood as the expected error on 50% and 90% of the sampled values, respectively. 
The .5-risk is equivalent to the mean absolute percentage error (MAPE).

The RMSE loss is defined as:

#+BEGIN_EXPORT latex
\begin{equation*}
\sum^n_{t}\sqrt{\frac{(y_t - y^*_{t})^2}{n}}
\end{equation*}
#+END_EXPORT


*** Results

Table [[taba1]] reports how each model performs as a function of the timespan of forecasting. 
The error inscreases as time passes, as is expected. All models perform well on the 24h (next-day) forecasting.

#+BEGIN_center
#+NAME: taba1
#+CAPTION: RMSE values by forecast span
|-----------------+---------------------|
|    Deep Factors |                RMSE |
|-----------------+---------------------|
|             24h | 0.18                |
|              3d |  2.36               |
|              7d |  1.83               |
|-----------------+---------------------|
|         Deep AR |                RMSE |
|-----------------+---------------------|
|             24h | 0.07                |
|              3d |  1.37               |
|              7d |  1.44               |
|-----------------+---------------------|
| Encoder Decoder |                RMSE |
|-----------------+---------------------|
|             24h |  0.22               |
|              3d |   0.36              |
|              7d |  1.04               |
|-----------------+---------------------|
#+END_center

Next, Figures [[forecastencdec]], [[forecastdeepfactors]] and [[forecastdeepar]] report the full prediction on the dev set timespan, with it's uncertainties.

#+NAME: forecastencdec
#+CAPTION: Forecast Results for Encoder Decoder Model 
#+ATTR_LaTeX: :height 0.3\textwidth :placement [H]
[[file:~/Dropbox/Mestrado/Intercement/paper_img/forecast_enc_dec.pdf]] 

#+NAME: forecastdeepfactors
#+CAPTION: Forecast Results for Deep Factors Model 
#+ATTR_LaTeX: :height 0.3\textwidth :placement [H]
[[file:~/Dropbox/Mestrado/Intercement/paper_img/forecast_deep_factors.pdf]] 

#+NAME: forecastdeepar
#+CAPTION: Forecast Results for Deep AR Model 
#+ATTR_LaTeX: :height 0.3\textwidth :placament [H]
[[file:~/Dropbox/Mestrado/Intercement/paper_img/forecast_deep_ar.pdf]] 

We now plot the predictions for 90 days after T of the models against it's true values, to evaluate the distribution of the predicted values.

#+BEGIN_center
#+ATTR_LaTeX: :height 0.3\textwidth :center
[[file:~/Dropbox/Mestrado/Intercement/paper_img/qq_deep_ar.pdf]] 
#+ATTR_LaTeX: :height 0.3\textwidth :center
[[file:~/Dropbox/Mestrado/Intercement/paper_img/qq_deep_factors.pdf]] 
#+ATTR_LaTeX: :height 0.3\textwidth :center
[[file:~/Dropbox/Mestrado/Intercement/paper_img/qq_enc_dec.pdf]] 
#+END_center

The Encoder-Decoder models seems to be able to best model the target distribution.

To evaluate the quality of the uncertainty measures, we shall use the .5 risk and .9 risk metrics. For each model 
we will compare the risks for the predictions of the next day, the next 3 days and the next 7 days. (The bigger the number the worse is the uncertainty).

#+BEGIN_center
#+NAME: taba 
#+CAPTION: Table with risks for each timespan forecast 
#+attr_latex: :environment tabular :width \textwidth :align lrr
|-----------------+---------+---------|
| Encoder Decoder | .5 risk | .9 risk |
|-----------------+---------+---------|
|             24h |   0.004 |   0.025 |
|              3d |   0.005 |    0.02 |
|              7d |   0.011 |   0.037 |
|-----------------+---------+---------|
|    Deep Factors | .5 risk | .9 risk |
|-----------------+---------+---------|
|             24h |   0.001 |   0.036 |
|              3d |   0.009 |   0.031 |
|              7d |   0.023 |   0.027 |
|-----------------+---------+---------|
|         Deep AR | .5 risk | .9 risk |
|-----------------+---------+---------|
|             24h |   0.009 |   0.004 |
|              3d |   0.018 |   0.008 |
|              7d |   0.044 |   0.016 |
#+END_center


As Table [[taba]] shows, the 3 models have good next-day predictions, with a low .5 risk. All risks increasingly increase as the forecast acts further in the future, as is expected.



** Conclusion

This work has applied Bayesian Deep Learning techniques to the modeling of uncertainty of cement strength prediction. The results of the RMSE measures are similar to state of the art results using
linear regression based techniques. We present results of uncertainty metrics using multiple models and time horizons, to serve as a benchmark for future work on this direction. 
We have not been provided with enough data to be comparable with most public datasets used to test Deep Learning models, with that much data, the Deep Learning approach is more resource intensive 
and prone to variance problems than the models used on previous work on this task. On the future however, with more data, the Deep Learning approach should be more prevalent on this industry.

#+BEGIN_EXPORT latex
\bibliographystyle{plain}
\bibliography{bibliografia}{}
#+END_EXPORT 
