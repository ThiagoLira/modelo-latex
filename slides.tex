%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolorthGenteee! To passando esse fds prolongado aqui em Aspen né (quem só conhece no winter recomendo super vir no fall)eme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}
\newcommand{\R}{\mathbb{R}}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage[brazilian]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
\usepackage{dirtytalk}
\usepackage{empheq}
\usepackage[many]{tcolorbox}
\usepackage{relsize}
\usetikzlibrary{positioning,backgrounds, fit, arrows.meta,shapes.arrows,shapes}
\usepackage{lmodern}
\usepackage{pgfgantt}
\usepackage{bm}
\usepackage[scale=2]{ccicons}
\usepackage{pgfplots}
\usepackage{array,colortbl,xcolor}
\usepgfplotslibrary{dateplot}
\usepackage{setspace}
\usepackage{etoolbox}
\usepackage{xspace}
\usepackage{tkz-euclide}
\usepackage{tikz}
\AtBeginEnvironment{quote}{\singlespacing}

% definitions
\input{definitions/colors}
\input{definitions/styles}

% new commands
\input{tiks/all_new_commands.tex}

\usepackage{latexsym}
\usepackage{amsmath}
% \usepackage{commath} 
\usepackage{amssymb}
\usepackage{mathtools}

\usetheme{metropolis}
\urlstyle{same}
\graphicspath{ {figuras/} }
%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{Modelos Sequencias para Regressão em Séries Temporais} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{Thiago Lira} % Your name
\institute[IME-USP] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Instituto de Matemática e Estatística - USP \\ % Your institution for the title page
\medskip
\textit{thlira@ime.usp.br} % Your email address
}
\date{\today} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Overview} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}


\section{Introdução}


\begin{frame}
\frametitle{Histórico}

\begin{itemize}
\item A durabilidade e vida útil do cimento tem sido o problema mais importante enfrentado
pela indústria de construção civil nas últimas décadas.
\item Prever a resistência do Cimento ainda durante a sua produção amenizaria
  custos da ordem de bilhões de dólares para a indústria.
\item Avanços recentes no campo de Deep Learning forneceram ferramentas que
  melhoram o estado da arte para modelagem de Séries Temporais.
\end{itemize}

\end{frame}



\begin{frame}
\frametitle{Etapas da Produção de Cimento}
%%% slide cimento foto do processo com numeros DECORA FDP
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{cimento.png}
\caption{Representação das diversas etapas da produção de cimento}
\end{figure}
\end{frame}

\section{Análise dos Dados}

\begin{frame}
  \frametitle{Entradas presentes nos Dados}
%%% plot com numero de entradas faltando nos dados 
\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{slides_dados_pct}
\caption{Dados faltantes em cada coluna dos dados de Expedição de Cimento}
\end{figure}

\end{frame}

\begin{frame}
%%% slide apenas do RC3,RC7 e RC28
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{slides_dados}
\caption{Dados que serão usados para o problema de Regressão}
\end{figure}

\end{frame}


\begin{frame}
%%% slide com tabela de correlação
\frametitle{Tabela de Correlação}
\begin{table}[H]
\centering
\begin{tabular}{l|lll}
\cline{2-4}
\textbf{}                  & \multicolumn{1}{l|}{RC3} & \multicolumn{1}{l|}{RC7} & \multicolumn{1}{l|}{RC28} \\ \hline
\multicolumn{1}{|l|}{RC3}  & 1                        & 0.734201                 & 0.388973                  \\ \cline{1-1}
\multicolumn{1}{|l|}{RC7}  & 0.734201                 & 1                        & 0.484725                  \\ \cline{1-1}
\multicolumn{1}{|l|}{RC28} & 0.388973                 & 0.484725                 & 1                         \\ \cline{1-1}
\end{tabular}
\caption{Correlação entre Índices de Resistência dos dados de
  Expedição de Cimento}
\label{corr3728}
\end{table}

\end{frame}

\begin{frame}
%%% slide com analise FFT só do RC28
\frametitle{Análise de Periodicidade}
\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{FFT_RC28.png}
\caption{Análise Espectral para preditor RC28}
\end{figure}

\end{frame}


\begin{frame}
  \frametitle{Definição do Problema}
%%% slide definindo aprendizado supervisionado/regressao
  \begin{itemize}
    \item O problema de modelar o índice de Resistência é um problema de
  \textbf{Aprendizado Supervisionado} de \textbf{Regressão}.
    \item Queremos modelar uma função que aprenda com um conjunto de dados $\{(x_1,y_1)
\dots , (x_n,y_n)\}$ e possa nos gerar anotações inéditas $y^*$ para novas
entradas $x^*$.

    \item O aprendizado busca em uma certa família de funções aquela que maximiza a
      verossimilhança de uma distribuição implícita $p'(y | x)$ aprendida pelo modelo.
    
  \end{itemize}
  

  
\end{frame}

\begin{frame}
  \frametitle{Definição do Problema}

  \begin{itemize}
\item Em um primeiro momento podemos criar triplas de treino $(RC3_i,RC7_i,RC28_i)$, e queremos
  encontrar uma função: \\ 
  \begin{empheq}[box=\tcbhighmath]{align}
  \mathnormal{f}(RC3_i,RC7_i) = \widehat{RC28}_i 
  \end{empheq}
    \end{itemize}

Mas desse jeito o modelo não estaria usando nenhuma informação temporal dos dados.
    
\end{frame}


\begin{frame}
 
  \frametitle{Séries Temporais}

  \begin{itemize}
\item Dados anotados temporalmente, normalmente em intervalos regulares. O passado de
uma série pode trazer informação para prever o seu futuro.
\item Agora nosso problema
de aprendizado supervisionado leva em conta diversas anotações passadas. A
função aprendida pelo modelo teria como entrada diversas anotações
subsequentes passadas. 
  \begin{empheq}[box=\tcbhighmath]{align}
  &\mathnormal{f}(\textbf{x}_{t} ,\textbf{x}_{t -1} , \dots, \textbf{x}_{t-T}) =
  \widehat{RC28_t} \\
  &\textbf{x}_t = (RC3_t,RC7_t)
  \end{empheq}
    \end{itemize}
  
  %%% slide definindo serie temporal
\end{frame}


\begin{frame}
 
  \frametitle{Séries Temporais: Reamostragem}

  \begin{itemize}
\item Os dados da Intercement não estavam anotados em intervalos regulares,
  então eles foram reamostrados em intervalos de 1 dia. Com dias
  vazios sendo preenchidos com zeros.
    \end{itemize}
  
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{slides_dados_antes_resample.png}
\caption{A Frequência de Amostragem dos dados não é regular}
\end{figure}
\end{frame}



\section{Modelos Escolhidos}

\begin{frame}
%%% slide mostrando rede neural (mesmo diagrama da quali)
  \frametitle{Redes Neurais}
  \begin{empheq}[box=\tcbhighmath]{align}
    \mathnormal{f}_i (x)=  a_i = \sigma(W_i*a_{i-1} + b_i) 
  \end{empheq}

\begin{figure}
  \centering
  \input{chapters/NN.tex}
  \label{fig:nn}
\end{figure}


\end{frame}


\begin{frame}
\frametitle{Modelos Sequenciais: RNNs}
%%% slide definindo uma LSTM
Função de recorrência:\\
\bigskip
\quad Para $t=\{1, \dots,T\}$ \\
 \begin{empheq}[box=\tcbhighmath]{align}
   \mathnormal{f}(\textbf{h}_{t-1},\textbf{x}_t) = \textbf{h}_t
\end{empheq}
\centering
\begin{figure}[H]
\resizebox{0.3\textwidth}{!}{
 \input{tiks/RNNSimplified.tex}
}
\caption{RNN genérica}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Modelos Sequenciais: RNNs}
%%% slide definindo uma LSTM 2
\begin{figure}[H]
  \input{tiks/RRNSimplifiedUnrolled.tex}
  \caption{RNN genérica através do tempo}
\end{figure}

\end{frame}


\begin{frame}
\frametitle{Modelos Sequenciais: LSTMs}
%%% slide definindo uma LSTM 2
  \resizebox{1\textwidth}{!}{
    \input{chapters/lstm.tex}
    }

\end{frame}

\begin{frame}
\frametitle{Modelos Sequenciais: LSTMs}

A LSTM tem seu fluxo de informação regulado por diversos sinais, que agem como
válvulas que permitem ao modelo aprender quanto é importante lembrar ou esquecer
de dados passados e presentes.

\begin{empheq}[box=\tcbhighmath]{align}
  f_t &= \sigma_g(W_fx_t + U_fh_{t-1} + b_f) \\
  i_t &= \sigma_g(W_ix_t + U_ih_{t-1} + b_i)\\
  o_t &= \sigma_g(W_ox_t + U_oh_{t-1} + b_o)
\end{empheq}
O valor da cécula de memória é então calculado e usado para calcular o novo
vetor de estado. 
\begin{empheq}[box=\tcbhighmath]{align}
c_t &= f_t \circ c_{t-1} + i_t \circ \sigma_c(W_cx_t + U_ch_{t-1} + b_c) \\
h_t &= o_t \circ \sigma_h(c_t)  
\end{empheq}


\end{frame}



\begin{frame}
\frametitle{Modelos Sequenciais: Encoder-Decoder}
%%% slide definindo uma LSTM 2
Redes Encoder-Decoder são modelos poderosos para criar \textit{representações}
de dados de entrada. A ideia é que esses modelos consigam gerar um vetor que
sumarize esses dados, e depois consiga reproduzi-los apenas sendo guiado por
esse vetor. A esse vetor se dá o nome de \textit{embedding}. 



\begin{figure}[H]
\centering
\input{chapters/encdec.tex}
\caption{ Diagrama de Rede Encoder-Decoder}

\end{figure}

\end{frame}

\begin{frame}
%%% slide com modelo Uber
\frametitle{Modelos Sequenciais: Encoder-Decoder-Forecaster}
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{uber.png}
\caption{Arquitetura do modelo Encoder-Decoder-Forecaster}
\end{figure}
\end{frame}

\begin{frame}
%%% slide com modelo Uber
\frametitle{Modelos Sequenciais: Encoder-Decoder-Forecaster}

\begin{itemize}
  \item Uma fase de pré-treino passa por todas as janelas de dados e usa uma
    arquitetura Encoder-Decoder para aprender uma representação vetorial que
    sumarize a janela de dados consumida pelo Encoder. 
  \item Para predições, usamos o Encoder para consumir uma janela de dados
    inédita e o \textit{embedding} gerado é usado como entrada de uma Rede Neural, a mesma então é
    treinada para prever a medida RC28 equivalente a última data vista na
    janela. 
  
  \end{itemize}

\end{frame}




\begin{frame}
  \frametitle{Machine Learning Bayesiano}
%%% slide inferencia bayesiana com a integral e os caralho
No caso Bayesiano considera-se uma distribuicao de probabilidade a priori
$p(W)$ para todos os parametros. \\  

Os dados não são considerados variáveis aleatórias e sim obvervacoes e então
usamos a Lei de Bayes para calcular a probabilidade posterior dos valores dos
parâmetros dado que os dados foram observados. Também usamos no cálculo a
\textit{verossimilhanca} $p(Y | X, W)$. \\  

\begin{empheq}[box=\tcbhighmath]{align*}
  p(\theta | X,Y) = \frac{p(Y| X,W) p(W)}{p(Y)}  
\end{empheq}
\end{frame}
\begin{frame}
  \frametitle{Machine Learning Bayesiano}

Finalmente, para realizar uma inferência devemos integrar por toda a
distribuição $p(W)$ marginalizando esse parâmetro. \\
Se por exemplo queremos uma nova anotação $y^*$ para um novo dado $x^*$:
\begin{empheq}[box=\tcbhighmath]{align*}
p(y^* | x^* , X,Y) = \int  p(y^* | x^*,\theta) p(W | X,Y)  dW 
\end{empheq}

Essa integral é geralmente intratável pela dificuldade de se calcular
analiticamente $p(W | X,Y)$ e portanto será usada uma
técnica que aproxima uma inferência Bayesiana numericamente em redes neurais, o Monte Carlo
Dropout.
\end{frame}

\begin{frame}
  \frametitle{Dropout como Regularizador}
  \centering
%%% slide com dropout imagem da quali
  \resizebox{1\textwidth}{!}{
      \input{chapters/dropout.tex}
    }

    O Dropout força a rede neural a distribuir o aprendizado por todos os neurônios.
    
\end{frame}

\begin{frame}
  \frametitle{Dropout como Inferência Bayesiana}
  %%% slide com dropout imagem da quali

  É demonstrado que o Dropout pode ser interpretado como considerar uma
  distribuição de probabilidade em todos os parâmetros do modelo. Vamos usar
  como exemplo uma rede neural com uma camada oculta e dropout aplicado na
  camada de entrada, e na camada oculta. \\ 

  \begin{empheq}[box=\tcbhighmath]{align*}
  y^* &= \hat{a}W_2 \\
          &= (a * diag(\hat{\epsilon}_2))W_2 \\
          &=   \sigma(\hat{x}W_1 + b) *(diag(\hat{\epsilon}_2)W_2) \\
          &=   \sigma(x^* (diag(\hat{\epsilon}_1)W_1) + b) * (diag(\hat{\epsilon}_2)W_2) \\
  \end{empheq}

  
\end{frame}



\begin{frame}
  \frametitle{Dropout como Inferência Bayesiana}
  %%% slide com dropout imagem da quali
  Onde cada vetor $\epsilon_k$ é uma realização de um vetor de 0s e 1s seguindo
  uma distribuição de Bernoulli com probabilidade $p_k$ em cada posição: \\
  \begin{empheq}[box=\tcbhighmath]{align*}
    \hat{\epsilon}^{i}_k \sim Bernoulli(p_k)
  \end{empheq}
\end{frame}



\begin{frame}
  \frametitle{Dropout como Inferência Bayesiana}
  %%% slide com dropout imagem da quali
  Definimos uma distribuição $\mathnormal{q}(W)$ de onde podemos amostrar
  valores de $W$: \\  
  \begin{empheq}[box=\tcbhighmath]{align*}
    \hat{W} &\sim \mathnormal{q}(W) \\
    \hat{W_1} &:= diag(\hat{\epsilon}_1)W_1 \\
    \hat{W_2} &:= diag(\hat{\epsilon}_2)W_2
  \end{empheq}

  
Então podemos escrever a saída da rede neural como: \\
  \begin{empheq}[box=\tcbhighmath]{align*}
        y^*  =   \sigma(x \hat{W}_1 + b) * \hat{W}_2 =:
        \mathnormal{f}^{\hat{W}_1,\hat{W}_2,b}(x^*) 
\end{empheq}
\end{frame}

\begin{frame}
  \frametitle{Dropout como Inferência Bayesiana}
  %%% slide com dropout imagem da quali
  Considerando então os parâmetros do modelo como provenientes de uma
  distribuição de probabilidade, o uso de Dropout em todas
  as camadas da rede neural é uma aproximação por Monte Carlo da integral da
  Inferência Variacional: \\
  
  \begin{empheq}[box=\tcbhighmath]{align*}
    p(y^* | x^* , X,Y) &\approx  \int p(y^* | x^*,W)q(W)dW \\
                       &\approx \frac{1}{T}\sum^T_{T=1}p(y^*| x^*, \hat{W} )
  \end{empheq}

\end{frame}

\begin{frame}
  \frametitle{Dropout como Inferência Bayesiana}
  %%% slide com dropout imagem da quali

  Dessa maneira, podemos estimar os dois primeiros momentos das predições de
  maneira simples: \\
  
  \begin{empheq}[box=\tcbhighmath]{align*}
   \widetilde{\mathbb{E}}[y^*] &= \frac{1}{B}\sum^B_{B=1}\hat{y}^*_{(B)} \\ 
   \widetilde{\mathit{Var}}[y^*]  &= \frac{1}{B}\sum^B_{B=1}(\hat{y}^*_{(B)} - \bar{y}^*)^2 
  \end{empheq}

\end{frame}


\begin{frame}
%%% slide do modelo da uber com zoom
\end{frame}

\begin{frame}
%%% formulas de variancia no modelo uber

  Finalmente, estimamos a incerteza do modelo usando o MC Dropout. E o ruído
  inerente usando a variância amostral de um conjunto de dados de validação:\\ 

  \begin{empheq}[box=\tcbhighmath]{align*}
   \widetilde{\mathit{Var}}[f^W(x^*)]  &=
   \frac{1}{B}\sum^B_{B=1}(\hat{y}^*_{(B)} - \hat{y}^*)^2 \\ 
   \widetilde{\sigma}^2 &= \frac{1}{V}\sum^V_{V=1}(y'_v - f^W(x'_v))^2 
  \end{empheq}

\end{frame}

\section{Resultados}

\begin{frame}
  %%% Divisão do dataset
  \frametitle{Preparo dos Dados}
  \begin{itemize}
    \item Dados foram divididos entre \textbf{treino} e \textbf{validação}.
      Usamos os dados anotados de 2008 até 2015 para treinar os modelos e de
      2016 em diante para valida-los, testando a sua capacidade de
      generalização.
      
    \item Todos os dados foram transformados segundo a equação $\textbf{z} =
      \frac{\textbf{x} - \mu}{\sigma}$, para que ficassem com média 0 e
      variância unitária. Isso evitou efeitos indejados no treinamento dos modelos.
      
    \end{itemize}
    \begin{figure}[H]
  \centering
  \includegraphics[width=0.9\columnwidth]{split_2008-2015-2017RC3.png}
\end{figure}

\end{frame}


\begin{frame}
  %%% resultados nao sequenciais
\frametitle{Modelos Não-Sequenciais: Resultados}  
\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{exped_saco_2008-2012-2014RC28.png}
\end{figure}
\end{frame}


\begin{frame}
  \frametitle{Preparo dos Dados: Modelos Sequenciais}
  %%% resultados modelos não-sequenciais
  \begin{itemize}
  \item Para a modelagem sequencial, os dados são separados em janelas. O tamanho
    dessas janelas é um hiper-parâmetro, no caso, usamos janelas de 30 dias,
    sempre pulando 1 dia entre o começo de cada janela.
    
  \item Cada exemplo de treinamento será então uma sequencia de 30 dias de dados dos
    índices RC3 e RC7, e apenas uma anotação do índice RC28.
  \item A ideia é que o modelo tenha uma predição para a medida RC28 \say{atual} de um lote de cimento
    tendo como guia as medidas RC3 e RC7 atuais desse lote (exatamente como no
    caso dos modelos não-sequenciais), bem como as medidas
    dos ultimos 30 dias desses valores para outros lote.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Preparo dos Dados: Modelos Sequenciais}
    \begin{figure}[H]
  \centering
  \includegraphics[scale=0.3]{slides_windows}
  \caption{Janelas subsequentes de 30 dias retiradas dos dados.}
\end{figure}
\end{frame}


\begin{frame}
%%% resultados modelo uber
\frametitle{Rede Encoder-Decoder-Forecaster: Resultados}  
\begin{figure}[H]
\centering
\includegraphics[width=0.9\columnwidth]{exped_saco_2008-2015-2017encdecRC28.png}
\caption{Resultados da rede Encoder-Decoder-Forecaster na tarefa de regressão do índice RC28}
\label{fig:3nseq}
\end{figure}
\end{frame}

\begin{frame}
%%% conclusão até agora
\end{frame}

\begin{frame}
  \frametitle{Cronograma de Pesquisa}
%%% direção futura imagem do cronograma
\begin{figure}[H]
  \resizebox{1\textwidth}{!}{
     \begin{ganttchart}[%Specs
     y unit title=0.5cm,
     y unit chart=0.7cm,
     vgrid,hgrid,
     title height=1,
%     title/.style={fill=none},
     title label font=\bfseries\footnotesize,
     bar/.style={fill=blue},
     bar height=0.7,
%   progress label text={},
     group right shift=0,
     group top shift=0.7,
     group height=.3,
     group peaks width={0.2},
     inline]{1}{24}
     % labels


     % Titulo da tabela e quantos quadradinhos vai ter no total
     \gantttitle{2018-2019 - Mestrado}{24}\\

     % primeira subdivisão de quadradinhos em anos, quantos quadradinhos tem por
     % ano(precisa somar o mesmo que o valor
     % que vc definiu em cima)
    \gantttitle[]{2018}{12}                 
    \gantttitle[]{2019}{12} \\              

    %% subsubdivisão de quadradinhos em meses, tb precisa somar o total
    %% desse jeito cada quadradinho conta como 1 semana de trabalho
    %% 4 quadradinhos por mes
    \gantttitle{Outubro}{4}                
    \gantttitle{Novembro}{4}
    \gantttitle{Dezembro}{4}
    \gantttitle{Janeiro}{4}
    \gantttitle{Fevereiro}{4}
    \gantttitle{Março}{4}\\


    %%% primeiro valor nos colchetes -> nome da tarefa
    %% segundo e terceiro valores -> de qual quadradinho até qual quadradinho a
    %% tarefa vai, de acordo com o numero de quadradinhos que vc definiu la em
    %% cima
    \ganttbar[inline=false]{Tarefa 1}{1}{4}\\
    \ganttbar[inline=false]{Tarefa 2}{2}{6}\\ 
    \ganttbar[inline=false]{Tarefa 3}{3}{8} \\
    \ganttbar[inline=false]{Tarefa 4}{5}{10} \\
    \ganttbar[inline=false]{Tarefa 5}{8}{12} \\
    \ganttbar[inline=false]{Tarefa 6}{12}{20} \\
    \ganttbar[inline=false]{Tarefa 7}{13}{21} \\
    \ganttbar[inline=false]{Tarefa 8}{14}{24} \\



    
    
  \end{ganttchart}
  }
\end{figure}
\end{frame}


\begin{frame}
  \frametitle{Cronograma de Pesquisa}
\begin{itemize}

\item[1: ] Refazer Experimentos sem o delay temporal, apenas com os dados
  de Expedição de Cimento
\item[2: ] Usar modelo para outros dados de séries temporais (e.g. clima)
\item[3: ] Nova compilação de resultados
\item[4: ] Uso de dados de outras etapas da produção de concreto simultaneamente
\item[5: ] Estudar alterações no modelo proposto pela Uber  
\item[6: ] Experimentos com mudanças propostas 
\item[7: ] Estudo e compilação de resultados
\item[8: ] Escrita da tese 


  
\end{itemize}

\end{frame}









\end{document}